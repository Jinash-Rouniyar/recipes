{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/weaviate-features/multi-vector/multi-vector-colipali-rag.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-vector RAG: Using Weaviate to search a collection of PDF documents \n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook will demonstrate how to use Weaviate's multi-vector feature to \n",
    "effectively index a collection of PDF documents in order to support textual \n",
    "queries against the contents of the documents, including both text and figures.\n",
    "\n",
    "In this demonstration we will be working with a dataset of the [top-40 most \n",
    "cited AI papers on arXiv](https://arxiv.org/abs/2412.12121) from the period \n",
    "2023-01-01 to 2024-09-30. \n",
    "\n",
    "We will be performing retrieval against this collection of PDF documents by \n",
    "embedding both the individual pages of the documents and our queries into the \n",
    "same multi-vector space, reducing the problem to approximate nearest-neighbor \n",
    "search on ColBERT-style multi-vectors under the MaxSim similarity measure. \n",
    "\n",
    "The approach we will be using to generate embeddings is outlined in the recent \n",
    "paper [ColPali: Efficient Document Retrieval with Vision Language Models](https://arxiv.org/abs/2407.01449). \n",
    "The paper demonstrates that it is possible to both simplify and \n",
    "speed up traditional approaches to preprocessing PDF documents for retrieval, \n",
    "which involves the use of OCR (Optical Character Recognition) software and \n",
    "separate processing of text and figures, by instead feeding images (screenshots) \n",
    "of entire pages to a Vision Language Model that produces a ColBERT-style embedding.\n",
    "\n",
    "![colipali pipeline](figures/colipali_pipeline.jpeg)\n",
    "\n",
    "Specifically, we will be using the publicly available model \n",
    "[ColQwen2-v1.0](https://huggingface.co/vidore/colqwen2-v1.0) to generate \n",
    "embeddings.\n",
    "\n",
    "## Retrieval example\n",
    "\n",
    "As an example of what we are going to build consider the following actual demo \n",
    "query and resulting PDF page from our collection (nearest neighbor):\n",
    "\n",
    "- Query: \"How does DeepSeek-V2 compare against the LLaMA family of LLMs?\"\n",
    "- Nearest neighbor:  \"DeepSeek-V2: A Strong Economical and Efficient \n",
    "Mixture-of-Experts Language Model\" (arXiv: 2405.04434), Page: 1.\n",
    "\n",
    "By inspecting the first page of the \n",
    "[DeepSeek-V2 paper](https://arxiv.org/abs/2405.04434) we see that it does indeed \n",
    "contain a figure that is relevant for answering our query:\n",
    "\n",
    "![deepseek efficiency](figures/deepseek_efficiency.jpeg)\n",
    "\n",
    "## Extension to Retrieval Augmented Generation (RAG)\n",
    "\n",
    "The above example gives us the most relevant pages to begin looking at in order \n",
    "to answer our query. Vision language models are now powerful enough that we can \n",
    "instead give the query and relevant pages to such a model and have it produce an \n",
    "answer to our query in plain text! \n",
    "\n",
    "In order to accomplish this we are going to feed the top results into the \n",
    "state-of-the-art VLM [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct).\n",
    "\n",
    "## Demonstration overview\n",
    "\n",
    "The demonstration will proceed through the following steps in order to set up a\n",
    " running retrieval example:\n",
    "\n",
    "1. Loading the ColQwen model from huggingface and adding convenience functions \n",
    "to vectorize images and queries.\n",
    "2. Load an example dataset of PDF pages from huggingface.\n",
    "3. Spinning up a local Weaviate server and creating a collection of \n",
    "bring-your-own multivectors.\n",
    "4. Querying the collection and displaying results.\n",
    "5. Setting up Qwen2.5-VL to support retrieval-augmented generation.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- The Python packages listed in the Pipfile in this directory.\n",
    "- A machine capable of running neural networks using 5-10 GB of memory.\n",
    "- A local instance of Weaviate version >= 1.29.0\n",
    "\n",
    "To install all dependencies as listed in the [Pipfile](https://github.com/weaviate/recipes/blob/main/weaviate-features/multi-vector/Pipfile) use `pipenv install` to set \n",
    "up the local environment for this notebook. \n",
    "\n",
    "The demonstration uses two different vision language models that both require \n",
    "several gigabytes of memory. See the documentation for each individual model and \n",
    "the general pytorch docs in order to figure out how to best run the models on \n",
    "your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.13.1' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# Load the ColQWEN model\n",
    "import torch\n",
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "import os\n",
    "\n",
    "# Get rid of process forking deadlock warnings.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# A convenience class to wrap the functionality we will use from\n",
    "# https://huggingface.co/vidore/colqwen2-v1.0\n",
    "class Colqwen:\n",
    "    def __init__(self):\n",
    "        \"\"\"Load the model and processor from huggingface.\"\"\"\n",
    "        # About a 5 GB download and similar memory usage.\n",
    "        self.model = ColQwen2.from_pretrained(\n",
    "            \"vidore/colqwen2-v1.0\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"mps\",  # or \"cuda:0\" if using a NVIDIA GPU\n",
    "            attn_implementation=\"eager\",  # or \"flash_attention_2\" if available\n",
    "        ).eval()\n",
    "        self.processor = ColQwen2Processor.from_pretrained(\"vidore/colqwen2-v1.0\")\n",
    "\n",
    "    # A batch size of one appears to be most performant when running on an M4.\n",
    "    # Note: Reducing the image resolution speeds up the vectorizer and produces\n",
    "    # fewer multi-vectors.\n",
    "    def multi_vectorize_image(self, img):\n",
    "        \"\"\"Return the multi-vector image of the supplied PIL image.\"\"\"\n",
    "        image_batch = self.processor.process_images([img]).to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            image_embedding = self.model(**image_batch)\n",
    "        return image_embedding[0]\n",
    "\n",
    "    def multi_vectorize_text(self, query):\n",
    "        \"\"\"Return the multi-vector embedding of the query text string.\"\"\"\n",
    "        query_batch = self.processor.process_queries([query]).to(self.model.device)\n",
    "        with torch.no_grad():\n",
    "            query_embedding = self.model(**query_batch)\n",
    "        return query_embedding[0]\n",
    "\n",
    "    def maxsim(self, query_embedding, image_embedding):\n",
    "        \"\"\"Compute the MaxSim between the query and image multi-vectors.\"\"\"\n",
    "        return self.processor.score_multi_vector(\n",
    "            [query_embedding], [image_embedding]\n",
    "        ).item()\n",
    "\n",
    "\n",
    "# Instantiate the model to be used below.\n",
    "colqwen = Colqwen()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a dataset from huggingface\n",
    "from datasets import load_dataset\n",
    "\n",
    "page_data = load_dataset(\"weaviate/arXiv-AI-papers-multi-vector\").with_format(\n",
    "    \"numpy\", columns=[\"colqwen_embedding\"], output_all_columns=True\n",
    ")[\"train\"]\n",
    "\n",
    "img = page_data[12][\"page_image\"]\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the embedding of images and queries works as intended.\n",
    "page_embedding = colqwen.multi_vectorize_image(img)\n",
    "print(page_embedding.shape)  # torch.Size([755, 128])\n",
    "\n",
    "queries = [\n",
    "    \"A table with LLM benchmark results.\",\n",
    "    \"A figure detailing the architecture of a neural network.\",\n",
    "]\n",
    "\n",
    "query_embeddings = [colqwen.multi_vectorize_text(q) for q in queries]\n",
    "print(query_embeddings[0].shape)  # torch.Size([20, 128])\n",
    "\n",
    "# The page matches the first query but not the second. Verify that the\n",
    "# similarity scores reflect this.\n",
    "print(colqwen.maxsim(query_embeddings[0], page_embedding))  # 13.4375\n",
    "print(colqwen.maxsim(query_embeddings[1], page_embedding))  # 9.5625"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure that you have weaviate >= 1.29.0 running locally.\n",
    "!docker run --detach -p 8080:8080 -p 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.29.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Pages collection that will hold our page embeddings.\n",
    "import weaviate\n",
    "import weaviate.classes.config as wc\n",
    "from weaviate.classes.config import Configure\n",
    "\n",
    "client = weaviate.connect_to_local()\n",
    "client.collections.create(\n",
    "    name=\"Pages\",\n",
    "    properties=[\n",
    "        wc.Property(name=\"page_id\", data_type=wc.DataType.INT),\n",
    "        wc.Property(name=\"dataset_index\", data_type=wc.DataType.INT),\n",
    "        wc.Property(name=\"paper_title\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"paper_arxiv_id\", data_type=wc.DataType.TEXT),\n",
    "        wc.Property(name=\"page_number\", data_type=wc.DataType.INT)\n",
    "    ],\n",
    "    vectorizer_config=[Configure.NamedVectors.none(\n",
    "        name=\"colqwen\",\n",
    "        vector_index_config=Configure.VectorIndex.hnsw(\n",
    "            multi_vector=Configure.VectorIndex.MultiVector.multi_vector()\n",
    "        )\n",
    "    )]\n",
    ")\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data into weaviate.\n",
    "import numpy as np\n",
    "client = weaviate.connect_to_local()\n",
    "pages = client.collections.get(\"Pages\")\n",
    "\n",
    "# Map of page ids to images to support displaying the image corresponding to a \n",
    "# particular page id.\n",
    "page_images = {}\n",
    "\n",
    "with pages.batch.dynamic() as batch:\n",
    "    for i in range(len(page_data)):\n",
    "        p = page_data[i]\n",
    "        page_images[p[\"page_id\"]] = p[\"page_image\"]\n",
    "\n",
    "        batch.add_object(\n",
    "            properties={\n",
    "                \"page_id\": p[\"page_id\"],\n",
    "                \"paper_title\": p[\"paper_title\"], \n",
    "                \"paper_arxiv_id\": p[\"paper_arxiv_id\"], \n",
    "                \"page_number\": p[\"page_number\"]\n",
    "                }, vector={\"colqwen\": p[\"colqwen_embedding\"]})\n",
    "        \n",
    "        if i % 25 == 0:\n",
    "            print(f\"Added {i+1}/{len(page_data)} Page objects to Weaviate.\") \n",
    "\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of retrieving relevant PDF pages to answer a query.\n",
    "import weaviate\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "query_text = \"How does DeepSeek-V2 compare against the LLaMA family of LLMs?\"\n",
    "\n",
    "query_embedding = colqwen.multi_vectorize_text(query_text).cpu().float().numpy()\n",
    "\n",
    "with weaviate.connect_to_local() as client:\n",
    "    pages = client.collections.get(\"Pages\")\n",
    "    response = pages.query.near_vector(\n",
    "        near_vector=query_embedding,\n",
    "        target_vector=\"colqwen\",\n",
    "        limit=10,\n",
    "        return_metadata=MetadataQuery(distance=True),\n",
    "    )\n",
    "    for i, o in enumerate(response.objects):\n",
    "        print(\n",
    "            f\"{i+1}) MaxSim: {-o.metadata.distance:.2f}, \"\n",
    "            + f\"Title: \\\"{o.properties['paper_title']}\\\" \"\n",
    "            + f\"(arXiv: {o.properties['paper_arxiv_id']}), \"\n",
    "            + f\"Page: {int(o.properties['page_number'])}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Qwen2.5-VL-3B-Instruct for generating answers from a query string \n",
    "# plus a collection of (images of) PDF pages.\n",
    "# Note: I had to install the transformers package using the command\n",
    "# pip install git+https://github.com/huggingface/transformers accelerate\n",
    "# in order to get this bleeding-edge model to work.\n",
    "import torch\n",
    "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
    "from qwen_vl_utils import process_vision_info\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "class QwenVL:\n",
    "    def __init__(self):\n",
    "        # Adjust the settings to your available architecture, see the link\n",
    "        # https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct for examples.\n",
    "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
    "            torch_dtype=torch.bfloat16,\n",
    "            device_map=\"mps\",\n",
    "            attn_implementation=\"eager\"\n",
    "        )\n",
    "\n",
    "        min_pixels = 256*28*28\n",
    "        max_pixels = 1280*28*28\n",
    "        self.processor = AutoProcessor.from_pretrained(\n",
    "            \"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
    "\n",
    "    def query_images(self, query, images):\n",
    "        \"\"\"Generate a textual response to the query (text) based on the information in the supplied list of PIL images.\"\"\"\n",
    "        # Preparation for inference.\n",
    "        # Convert the images to base64 strings.\n",
    "        content = []\n",
    "        for img in images:\n",
    "            buffer = BytesIO()\n",
    "            img.save(buffer, format=\"jpeg\")\n",
    "            img_base64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "            content.append({\"type\": \"image\", \"image\": f\"data:image;base64,{img_base64}\"})\n",
    "\n",
    "        content.append({\"type\": \"text\", \"text\": query})\n",
    "        messages = [{\"role\": \"user\", \"content\": content}]\n",
    "\n",
    "        text = self.processor.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        image_inputs, video_inputs = process_vision_info(messages)\n",
    "        inputs = self.processor(\n",
    "            text=[text],\n",
    "            images=image_inputs,\n",
    "            videos=video_inputs,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        inputs = inputs.to(\"mps\")\n",
    "\n",
    "        # Inference: Generation of the output.\n",
    "        generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n",
    "        generated_ids_trimmed = [\n",
    "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        return self.processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "\n",
    "# Instantiate the model to be used below.\n",
    "qwenvl = QwenVL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"# RAG examples\n",
    "\n",
    "Try this out yourself below.\n",
    "\n",
    "### Query 1: How did DeepSeek-V2 manage to outperform existing state of the art LLMs?\n",
    "\n",
    "\n",
    "The most relevant documents for the query \"How did DeepSeek-V2 manage to outperform existing state of the art LLMs?\" by order of relevance:\n",
    "\n",
    "1) MaxSim: 30.03, Title: \"DeepSeek-V2: A Strong Economical and Efficient Mixture-of-Experts Language Model\" (arXiv: 2405.04434), Page: 4\n",
    "2) MaxSim: 29.13, Title: \"DeepSeek-V2: A Strong Economical and Efficient Mixture-of-Experts Language Model\" (arXiv: 2405.04434), Page: 1\n",
    "3) MaxSim: 27.68, Title: \"DeepSeek-V2: A Strong Economical and Efficient Mixture-of-Experts Language Model\" (arXiv: 2405.04434), Page: 5\n",
    "\n",
    "The answer from Qwen2.5-VL-3B-Instruct based on these documents:\n",
    "```\n",
    "DeepSeek-V2 achieved this by optimizing the attention modules and Feed-Forward \n",
    "Networks (FFNs) within the Transformer framework, introducing Multi-head \n",
    "Latent Attention (MLA) and DeepSeekMoE architectures, and employing expert \n",
    "segmentation and shared expert isolation for higher potential in expert \n",
    "specialization. Additionally, it demonstrated strong performance with only 21B\n",
    "activated parameters, saving 42.5% of training costs, reducing the KV cache by \n",
    "93.3%, and boosting the maximum generation throughput to 5.76 times\n",
    "```\n",
    "\n",
    "### Query 2: Describe the figure on the front page of the paper \"Adding Conditional Control to Text-to-Image Diffusion Models\"\n",
    "\n",
    "The most relevant documents for the query \"Describe the figure on the front page of the paper \"Adding Conditional Control to Text-to-Image Diffusion Models\"\" by order of relevance:\n",
    "\n",
    "1) MaxSim: 31.99, Title: \"Adding Conditional Control to Text-to-Image Diffusion Models\" (arXiv: 2302.05543), Page: 1\n",
    "2) MaxSim: 25.26, Title: \"Stable Video Diffusion: Scaling Latent Video Diffusion Models to Large Datasets\" (arXiv: 2311.15127), Page: 1\n",
    "3) MaxSim: 24.71, Title: \"SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis\" (arXiv: 2307.01952), Page: 8\n",
    "\n",
    "The answer from Qwen2.5-VL-3B-Instruct based on these documents:\n",
    "```\n",
    "The figure on the front page of the paper \"Adding Conditional Control to \n",
    "Text-to-Image Diffusion Models\" is titled \"Figure 1: Controlling Stable \n",
    "Diffusion with learned conditions.\" It illustrates how users can control the \n",
    "image generation of large pretrained diffusion models using learned \n",
    "conditions, such as Canny edges, human pose, and other attributes.\n",
    "\n",
    "Here's a detailed description of the figure:\n",
    "\n",
    "### Figure 1: Controlling Stable Diffusion with learned conditions\n",
    "\n",
    "#### Top Row:\n",
    "1. **Input Canny edge**: A black-and-white sketch of a deer.\n",
    "2. **Default**: The default output without any additional conditions.\n",
    "3.\n",
    "```\n",
    "### Query 3: Why do we need the retrieval step when performing retrieval augmented generation?\n",
    "\n",
    "The most relevant documents for the query \"Why do we need the retrieval step when performing retrieval augmented generation?\" by order of relevance:\n",
    "\n",
    "1) MaxSim: 24.53, Title: \"Retrieval-Augmented Generation for Large Language Models: A Survey\" (arXiv: 2312.10997), Page: 2\n",
    "2) MaxSim: 23.41, Title: \"Retrieval-Augmented Generation for Large Language Models: A Survey\" (arXiv: 2312.10997), Page: 1\n",
    "3) MaxSim: 21.06, Title: \"Retrieval-Augmented Generation for Large Language Models: A Survey\" (arXiv: 2312.10997), Page: 4\n",
    "\n",
    "The answer from Qwen2.5-VL-3B-Instruct based on these documents:\n",
    "\n",
    "```\n",
    "The retrieval step is necessary in Retrieval-Augmented Generation (RAG) because \n",
    "it allows the model to access and utilize external knowledge from databases or \n",
    "other sources. This external knowledge can provide context, enhance the accuracy \n",
    "of the generated response, and help the model understand the user's query \n",
    "better. By incorporating this external knowledge, RAG can improve its \n",
    "performance on tasks that require domain-specific knowledge or require \n",
    "continuous updates based on new information.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "from weaviate.classes.query import MetadataQuery\n",
    "\n",
    "# Some example queries. Answering a query takes about a minute on an M4 Macbook.\n",
    "\n",
    "# query_text = \"How did DeepSeek-V2 manage to outperform existing state of the art LLMs?\"\n",
    "# query_text = \"Describe the figure on the front page of the paper \\\"Adding Conditional Control to Text-to-Image Diffusion Models\\\"\"\n",
    "\n",
    "query_text = \"Why do we need the retrieval step when performing retrieval augmented generation?\"\n",
    "query_embedding = colqwen.multi_vectorize_text(query_text).cpu().float().numpy()\n",
    "\n",
    "with weaviate.connect_to_local() as client:\n",
    "    pages = client.collections.get(\"Pages\")\n",
    "    response = pages.query.near_vector(\n",
    "        near_vector=query_embedding, \n",
    "        target_vector=\"colqwen\",\n",
    "        limit=3,\n",
    "        return_metadata=MetadataQuery(distance=True)\n",
    "    )\n",
    "    print(f\"The most relevant documents for the query \\\"{query_text}\\\" by order of relevance:\\n\")\n",
    "    result_images = [] \n",
    "    for i, o in enumerate(response.objects):\n",
    "        p = o.properties\n",
    "        print(\n",
    "            f\"{i+1}) MaxSim: {-o.metadata.distance:.2f}, \"\n",
    "            + f\"Title: \\\"{p['paper_title']}\\\" \"\n",
    "            + f\"(arXiv: {p['paper_arxiv_id']}), \"\n",
    "            + f\"Page: {int(p['page_number'])}\"\n",
    "        )\n",
    "        result_images.append(page_images[p[\"page_id\"]])\n",
    "    \n",
    "print(f\"\\nThe answer from Qwen2.5-VL-3B-Instruct based on these documents:\\n{qwenvl.query_images(query_text, result_images)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
