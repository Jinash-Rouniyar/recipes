{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYFo9Gdy-BSw"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/weaviate/recipes/blob/main/weaviate-features/multi-vector/multi-vector-colipali-rag.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7SB9B-G-BSy"
      },
      "source": [
        "# Multimodal RAG over PDFs using ColQwen2, Qwen2.5, and Weaviate\n",
        "\n",
        "This notebook demonstrates [Multimodal Retrieval-Augmented Generation (RAG)](https://weaviate.io/blog/multimodal-rag) over PDF documents.\n",
        "We will be performing retrieval against a collection of PDF documents by embedding both the individual pages of the documents and our queries into the same multi-vector space, reducing the problem to approximate nearest-neighbor search on ColBERT-style multi-vector embeddings under the MaxSim similarity measure.\n",
        "\n",
        "For this purpose, we will use\n",
        "\n",
        "- **A multimodal [late-interaction model](https://weaviate.io/blog/late-interaction-overview)**, like ColPali and ColQwen2, to generate\n",
        "embeddings. This tutorial uses the publicly available model\n",
        "[ColQwen2-v1.0](https://huggingface.co/vidore/colqwen2-v1.0) with a permissive Apache 2.0 license.\n",
        "- **A Weaviate [vector database](https://weaviate.io/blog/what-is-a-vector-database)**, which  has a [multi-vector feature](https://docs.weaviate.io/weaviate/tutorials/multi-vector-embeddings) to effectively index a collection of PDF documents and support textual queries against the contents of the documents, including both text and figures.\n",
        "- **A vision language model (VLM)**, specifically [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), to support multimodal Retrieval-Augmented Generation (RAG).\n",
        "\n",
        "Below, you can see the multimodal RAG system overview:\n",
        "\n",
        "<img src=\"https://github.com/weaviate/recipes/blob/main/weaviate-features/multi-vector/figures/multimodal-rag-diagram.png?raw=1\" width=\"700px\"/>\n",
        "\n",
        "First, the ingestion pipeline processes the PDF documents as images with the multimodal late-interaction model. The multi-vector embeddings are stored in a vector database.\n",
        "Then at query time, the text query is processed by the same multimodal late-interaction model to retrieve the relevant documents.\n",
        "The retrieved PDF files are then passed as visual context together with the original user query to the vision language model, which generates a response based on this information.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgo1k--1-deC"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "To run this notebook, you will need a machine capable of running neural networks using 5-10 GB of memory.\n",
        "The demonstration uses two different vision language models that both require several gigabytes of memory.\n",
        "See the documentation for each individual model and the general PyTorch docs to figure out how to best run the models on your hardware.\n",
        "\n",
        "For example, you can run it on:\n",
        "\n",
        "- Google Colab (using the free-tier T4 GPU)\n",
        "- or locally (tested on an M2 Pro Mac).\n",
        "\n",
        "Furthermore, you will need an instance of Weaviate version >= `1.29.0`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7iyXUkS3Aw8"
      },
      "source": [
        "## Step 1: Install required libraries\n",
        "\n",
        "Let's begin by installing and importing the required libraries.\n",
        "\n",
        "Note that you'll need Python `3.13`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "q8JvCewx984G"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "%pip install colpali_engine weaviate-client qwen_vl_utils\n",
        "%pip install -q -U \"colpali-engine[interpretability]>=0.3.2,<0.4.0\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "161XCtm33ctL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import userdata\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers.utils.import_utils import is_flash_attn_2_available\n",
        "from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor\n",
        "\n",
        "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
        "#from colpali_engine.models import ColPali, ColPaliProcessor # uncomment if you prefer to use ColPali models instead of ColQwen2 models\n",
        "\n",
        "import weaviate\n",
        "from weaviate.classes.init import Auth\n",
        "import weaviate.classes.config as wc\n",
        "from weaviate.classes.config import Configure\n",
        "from weaviate.classes.query import MetadataQuery\n",
        "\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from colpali_engine.interpretability import (\n",
        "    get_similarity_maps_from_embeddings,\n",
        "    plot_all_similarity_maps,\n",
        "    plot_similarity_map,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPYNXv3PJOCJ"
      },
      "source": [
        "## Step 2: Load the PDF dataset\n",
        "\n",
        "Let's start with the data.\n",
        "We're going to first load a PDF document dataset of the [top-40 most\n",
        "cited AI papers on arXiv](https://arxiv.org/abs/2412.12121) from Hugging Face from the period 2023-01-01 to 2024-09-30."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "cab9e9081adc4d288fae013271fbb4a3",
            "14a81148a257493bb4ff532f1c79c08c",
            "afc86040eb744cad907cb0f4aeebe21b",
            "ed2bf1fe3b454e53a0b2bf323f13eb34",
            "00e6ee311ac84dcabd0b1d0f030e5b10",
            "faf7f42484a648f392089e1dba9cbbb4",
            "4201fb49a2ec455f86da3346415e3862",
            "6da5255ea28a42f98db1d33ebcd8cef4",
            "af06dff047714d178cc5131078700869",
            "50ae94d626ce4f6dbee3fc94456d2e57",
            "e1cc7a6a0b654bf9b97cef316e218552",
            "b6bdcc01f033448da64afe62cae82c91",
            "7e50d4f93afa46c4a85f52a11b066f2d",
            "874d7068b03a473a89b83ccd393c3cad",
            "b926c8b10f8a4cdca227969a7492c264",
            "6fdcb005050e4bf0af388575b88cd2b0",
            "1fcb96857f67492b9171fcd814325256",
            "19128d43d5eb4fe4a2e875fe00fd4db6",
            "c9061b7d07754c028ed183a756dee1ea",
            "0c42b91c13da455ca3ad9488b638cd2d",
            "18b1c7c5f4aa40a2ac0ead5b0a850c35",
            "1aef46bf35fb4b81806e0e233f1f88c8",
            "b2210f0e9f6c4bed91ca677ce32ad29d",
            "dde7ae349927494dafdbd84ee539291a",
            "54d09f538ba34c39be0c8493f5669f4b",
            "bf7775d7eb134bd6bc770bd9117ef2aa",
            "a396cc765bc5423cbc195a328692bc77",
            "69ba515900b24eabb8f62e319370511d",
            "f2dc3f1c056d4838acae637134507f41",
            "e7aa5e1202e84eb79fc493cd82530206",
            "9d8158c2dc5b41afa449b14caedbec2b",
            "6832540e9fcf4bcaab0ead3c46ebad74",
            "246b767b98a54e73ba52ea7abf90e308"
          ]
        },
        "id": "t4D2Mg1u-BSz",
        "outputId": "091d96ab-d581-49f6-c2a6-60a9c3806d59"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cab9e9081adc4d288fae013271fbb4a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md:   0%|          | 0.00/530 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b6bdcc01f033448da64afe62cae82c91",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "n40_p10_images.parquet:   0%|          | 0.00/201M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b2210f0e9f6c4bed91ca677ce32ad29d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/399 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "dataset = load_dataset(\"weaviate/arXiv-AI-papers-multi-vector\", split=\"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjzOr9ENJel1",
        "outputId": "73c4e557-8761-41d5-f4b6-d92fcda56059"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['page_id', 'paper_title', 'paper_arxiv_id', 'page_number', 'colqwen_embedding', 'page_image'],\n",
              "    num_rows: 399\n",
              "})"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uxXu-fxoloJ"
      },
      "outputs": [],
      "source": [
        "dataset[398]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6uElnXHJlI0"
      },
      "source": [
        "Let's take a look at a sample document page from the loaded PDF dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "60QabGH4JikX",
        "outputId": "8a0ca26e-f17c-4094-8776-640980b62361"
      },
      "outputs": [],
      "source": [
        "display(dataset[289][\"page_image\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Retrieved page](./figures/retrieved_page.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQ9xfSazJHtp"
      },
      "source": [
        "## Step 3: Load the ColVision (ColPali or ColQwen2) model\n",
        "\n",
        "The approach to generate embeddings for this tutorial is outlined in the paper [ColPali: Efficient Document Retrieval with Vision Language Models](https://arxiv.org/abs/2407.01449). The paper demonstrates that it is possible to simplify traditional approaches to preprocessing PDF documents for retrieval:\n",
        "\n",
        "Traditional PDF processing in RAG systems involves using OCR (Optical Character Recognition) and layout detection software, and separate processing of text, tables, figures, and charts. Additionally, after text extraction, text processing also requires a chunking step. Instead, the ColPali method feeds images (screenshots) of entire PDF pages to a Vision Language Model that produces a ColBERT-style multi-vector embedding.\n",
        "\n",
        "<img src=\"https://github.com/weaviate/recipes/blob/main/weaviate-features/multi-vector/figures/colipali_pipeline.jpeg?raw=1\" width=\"700px\"/>\n",
        "\n",
        "There are different ColVision models, such as ColPali or ColQwen2, available, which mainly differ in the used encoders (Contextualized Late Interaction over Qwen2 vs. PaliGemma-3B). You can read more about the differences between ColPali and ColQwen2 in our [overview of late-interaction models](https://weaviate.io/blog/late-interaction-overview).\n",
        "\n",
        "Let's load the [ColQwen2-v1.0](https://huggingface.co/vidore/colqwen2-v1.0) model for this tutorial."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_KJSNaTMJZgH"
      },
      "outputs": [],
      "source": [
        "# Get rid of process forking deadlock warnings.\n",
        "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXzR5mJ1GZe-",
        "outputId": "8281e761-8053-4c93-d1cb-ed4d6222feae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda:0\n",
            "Using attention implementation: eager\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available(): # If GPU available\n",
        "    device = \"cuda:0\"\n",
        "elif torch.backends.mps.is_available(): # If Apple Silicon available\n",
        "    device = \"mps\"\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "\n",
        "if is_flash_attn_2_available():\n",
        "    attn_implementation = \"flash_attention_2\"\n",
        "else:\n",
        "    attn_implementation = \"eager\"\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Using attention implementation: {attn_implementation}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598,
          "referenced_widgets": [
            "ea1775bfd6714dcaae3cda4dfd4c8731",
            "49ca4910010848dbb5298eb7e9e2bade",
            "0e14d0b408ec4ac195784c1c28b820a0",
            "d0ea5691f1384bd7b56ef12522480e67",
            "82d0061090bb4f1b9fe6cdd77370458a",
            "7574fde63d6343cbb27b635e5776d2db",
            "3756331938b54f41954eb51da8b8bd94",
            "09b819f9e3f646bdae557c5a86af82b5",
            "22d9d2deecfc481191d9328fff859d19",
            "d5b86199cd6b45c1b380e311c553e385",
            "78f284f03a884213840766104b51f97c",
            "1088a90ec3aa4b1caf836dc234f2b456",
            "3bc57bee9e6a45f68c16735d9dd54f61",
            "8ca3e988cade4a709292ce0c3e9c30b3",
            "907f9080bccf4ffcb5726796f3636420",
            "f57ff563f0e94ae0b7f58e1943e09182",
            "22f52f64b845464e8c6bcc09aa08cc25",
            "886b799b2c6e4bfd8c2a6bbfaf5d5b35",
            "a7bbf99c2f554035a71ec4dfd6cd92dc",
            "a1636389e08847ffb39a2603c963abc3",
            "e380852632a2432e953129a2a7c097d2",
            "79e690c4ab4747868b993517251c2665",
            "b43f469811734215b24addd688c42c5b",
            "36323ae1cff74c2a9620cd62f71fe4b3",
            "6bcffd6aaea74a86a6403addef74c282",
            "1a69cc14f4e44c6c96dd320ef15724a2",
            "eeff2a92853642b4978f279695196583",
            "9d9d6087c7c640ca8300ee85571c5603",
            "2a1c6e2e5aec4e568d4e818a53d55531",
            "18176c3cf8384b8aba25e68c47e32ff2",
            "428694f8506841aaaa0cf183cde1928e",
            "2724955089d4416f934f10a0f0354e22",
            "8b7021636c6640f496f20e81ac4d1035",
            "0e7a11f606664207afd856b3001c0762",
            "9f3d5de87d33435cafab4cb2d36b9573",
            "5b901a6f107e4395889a81744d8478ce",
            "13b3b21d623c4ca282b6add944e6ff05",
            "c6ddb4047f054c6f9aa168c4a6dceef2",
            "bd1f1f850dcd473fa54ee37163e04893",
            "36445d9540034e17b0e3f7ce9bc8fd46",
            "8c340010d9bc4064b8419c29b037f7c9",
            "ef3dcc862ccb46cb933e6ebb534cc5f4",
            "54599fe4b86248608058722722b6d0bc",
            "41ed7b17a93f470597f9b5a21ad325ea",
            "1b6344c68d274dff9ac9b8b07d5a3137",
            "65f8177c4944436a94234187623f94ff",
            "1abf3c97da6248bb9f8f4cfd31376395",
            "0a45cd3110174d2d9a14221b1b6483ba",
            "eed538ae96f8402c8d1765c220e8e4ce",
            "165a3cc041ee4e15a0d23c1ea1007f10",
            "67713ba132f143578d494fd32d0e2bf7",
            "1ba6eb75c2c741a08e0ef1e96017574a",
            "df9fbc4eab194016a4d19fdb8e1a45f0",
            "b26855e085bc420a991bac2dcc42e761",
            "9519df23242a46be8027b394f247ef85",
            "edb51e39f57d43cb8b6fb40793cd5dd9",
            "0708cedb819b4acd9b7e58ac2a826b64",
            "9545dc27684043d59843d6d0740ba112",
            "0fdd8283c618443e8cf878baeb0723c7",
            "962468e561164c86acc1ae01404f5f1a",
            "972d298d631c464f836542c825bdaf4f",
            "135d25983cb94ab58f1c45f062966bed",
            "73ff84a10ca54aa9b310a8c80a543c29",
            "18b23be317604fc994dbd67a78e94938",
            "2d460e2e3e5843c395f2b147e0a8c875",
            "a4d4c5d6be474d9eadb17b88b99bf3f4",
            "bd0161651a0d4dd4985112fbe3e5743f",
            "b525316d69c04d468c57b4d9f2958178",
            "a77955cee80f4582b5360b79508dcf7b",
            "5b437383152240af803dd1d5449fd067",
            "6da0fb1e552d4acda0b5ad2917ed770a",
            "671787771618445882ccfff69fc11784",
            "3b30bcb491d84449b5689a7f994e1170",
            "0a181d7a37114d4fa0839a6d3adaf021",
            "c965415950474c389787a336331f8c77",
            "6df812456c3f432580c0b8dd48bd00bc",
            "a7936dfd4bf14207af431656b79f5fe9",
            "0ced154524164ad7aa3f4da6016047d2",
            "d80fb0c0b6e84298922b9e97f5dd91fc",
            "42f172efa21940648c262899e6663b3c",
            "22192f9c8ecf4c57b2340b346076366c",
            "cf8f0b87839c4fed840ab933d2d95936",
            "4770ceebeb8b48a685a094e767df2770",
            "b4d70e6880af445ba1d483a7ce9fa756",
            "75713ee96e35451892b3c1116e413cd7",
            "f2935ae364854044ae39ae51691eded4",
            "174bba9152f64473b853c04ade6db1a1",
            "92d6c1f7ab904cde86610d9293a50691",
            "c6cd8c9bf7bf476c9686a78d5bf21964",
            "c60c7ff63aa54705ba1b719f24ca3457",
            "bda89d684e0f475ba3a91a86a9046103",
            "af10ca95985844d9945ca607bd4f2eae",
            "6e5349bd75374dc4a16f0a19f6300481",
            "c12d30b97aed490796badc988c336253",
            "920e6bdfcf9044b7965d45a6b0dde94f",
            "810be63305804905b2c78c475a023516",
            "7732537f20b1483c81d4a97c00053f17",
            "d00ebe13c0e24e1fa3360bc416634f0f",
            "7232e26085344b28b4c502ffbd5fc91f",
            "1e1eb9c5b8f743e6bc8ab8774e5e88b4",
            "f376d4e9f62e4ccf9723f86f95c0dc5b",
            "c2a6a33efa214964adac26978abf7c9d",
            "b56c5b26e68a43c4b766e2e56b214550",
            "cd5397f1de5f49789f08a8d4548d9d55",
            "30971e675bf4401f87f63b38eb7966e7",
            "3f06fb3172404341ac33987ca6d0502e",
            "a8cdacaee6d4447fbf3f50e5524e8caa",
            "189e24333a90479393d7d8501732ea3c",
            "6298a353a7da49a2979ee79d21190026",
            "0cdb525c848547cbb62a117a094b86cd",
            "28849c9657b147e7b332c9b432d108e9",
            "75bf428986594e9a9b76071c4809236b",
            "dee0893341de4ce488912abb12733583",
            "9175f24f0b6c4b36962595ec7e92c950",
            "66a555d657ef4728a26b246a3122449f",
            "106e9871ec174e49b9ff14797247001c",
            "920c522dc10a45e8bf4df118b76b95a5",
            "64a5401f033142c0bd75684107656e4b",
            "750d96cef62648af9cfc000c36a85c9a",
            "211302692f5745c9b5296626e0a1451a",
            "6623ad565a2e441889ea132ca9e1a857",
            "88833cd9daa8428a9fb23910b3a7300c",
            "43eaa53f67284c9f970def0fc9974c0f",
            "018e82a145534f8db3b6e4ec77ca66b4",
            "2c1eb53c8e9d4d6abcc007dc10e2ac7d",
            "24d2fcc8910444e7836cc045de745054",
            "8648458e93b9466490eba1e99021f128",
            "ba22a086eb324497a881cf0a4cba8743",
            "f293880bef944711ac74aa3eb744e1bd",
            "ba8e994a5668441c81a1068c91f392fa",
            "f0e5f62e41b54efcacebc880565dbed6",
            "0365200d3cff4b778253954037113806",
            "d7d42c12e5544e44aec0cf0dc0ba062d",
            "7ac0cd0d4d6c44e0909853be444db988",
            "ddb710ee4b4f4a0e8df554af5541b649",
            "4c568057ec574e66b0bace1e3b49e716",
            "68ee9eda015749c3b9acb43939e4c898",
            "e96fdff0158840ba8eb4ab9c45c87d0a",
            "3a4139fd30764e1daf37e3789c1f0a2b",
            "7859cb3d35744f7b8d39fc2eeb16f0f1",
            "50ad46e90de44d2486d7a4978e1610ac",
            "642775440ff94b5798105edb7cbdbba4",
            "18c108a9cda546feac4936ef7edbe970",
            "306bb944a7a04d958de4565450f09ae6",
            "69bed8293c0b4729b9eaf541fe9c0352",
            "a33cb6537db14e5c825df570af297fb8",
            "4d462eb010d34d29b6f9bea44516f0ef",
            "8d72cfd607414d6ba00a9dee103f093c",
            "956be423763040f6aa69754ad04f6409",
            "90784a1c166c4725b0c4517f1308d321",
            "394727f241c84c06a13a5b0756783860",
            "5d6e560dd0694bc68a57aaf2d376714e",
            "09d91bb57fb547eab660d84b3a07dbe7",
            "2be4e29e41684e11b079a19e96136bdc",
            "77ee665992d04ae48a0acc0e02ff1187",
            "6525f0a3ff2f41de944e25aa28a4b449",
            "d4b1a92b91d6489992cbc332e1e30740",
            "af076222460a4053b91989fdac7da9d3",
            "0688cadad67e4e9aa764d7f8e2452d52",
            "1cbc030ea83b4f15be89875a9d3f54ad",
            "eb25eeaf41f64e80a2c80c5353c234b1",
            "2b1bf259b9bb45c09086f3f28db98d0c",
            "662e54d0083441588e3320aa7949aa97",
            "c4ffe5a629ee403c8dd0022371643987",
            "332ea81e590a4baf83bc03b29f0a246d",
            "59b6814d0e5c45888a4424490f7cd544",
            "2f870644d22741ffb3f6dc3d6de0d17f",
            "7ed5a276536b4b3585d43dc5a541bc40",
            "065cb805ac02480fba3afc7d4a520929",
            "0792f7fc362141969a88caf278b37a29",
            "185e297eaf724d8284f5b5db20cde57b",
            "2e4cb196c3ac4f84ad9b80d16bc47d46",
            "3a0b5efa1f2d422d893a649e09bfbb22",
            "eaea8aa435704d969dc830b55c541730",
            "460168053a4644bd81a382e70c4706ba",
            "099950d375f34fa68cc5409c398153c6",
            "94329030b6d9425a9bea2b34ce77735d",
            "806f202d7b414059b56fcbe45d0c5b63",
            "5758f5a7bacc4a05bcc25fb3ab1007ba",
            "e2ba792ea67d491498a4dc93ef5400e5",
            "52660a10572b4922ba5f7e94e4cfd499",
            "29d7eebba86d491db964d7b56000314f",
            "e078da2b3b664c4cb3f9799244dba987",
            "0ee6d07e07b849ebbb74f817ab40edaa",
            "1f2f80099ae54514ae57516c9668cc92",
            "d78ad9b8193b4cbdbbac832d1f2d8d7e",
            "e61dea28618e4153ab732d9f55495b96"
          ]
        },
        "collapsed": true,
        "id": "p9dKNgVTn-Gh",
        "outputId": "fdb792cf-04fc-4d57-89b2-434dd2460d9f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea1775bfd6714dcaae3cda4dfd4c8731",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_config.json:   0%|          | 0.00/728 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1088a90ec3aa4b1caf836dc234f2b456",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b43f469811734215b24addd688c42c5b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0e7a11f606664207afd856b3001c0762",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b6344c68d274dff9ac9b8b07d5a3137",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edb51e39f57d43cb8b6fb40793cd5dd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.85G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd0161651a0d4dd4985112fbe3e5743f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0ced154524164ad7aa3f4da6016047d2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/74.0M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c6cd8c9bf7bf476c9686a78d5bf21964",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1e1eb9c5b8f743e6bc8ab8774e5e88b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "28849c9657b147e7b332c9b432d108e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "88833cd9daa8428a9fb23910b3a7300c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7d42c12e5544e44aec0cf0dc0ba062d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "306bb944a7a04d958de4565450f09ae6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/392 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77ee665992d04ae48a0acc0e02ff1187",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/613 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59b6814d0e5c45888a4424490f7cd544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "video_preprocessor_config.json:   0%|          | 0.00/54.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94329030b6d9425a9bea2b34ce77735d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name = \"vidore/colqwen2-v1.0\"\n",
        "\n",
        "# About a 5 GB download and similar memory usage.\n",
        "model = ColQwen2.from_pretrained(\n",
        "    model_name,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=device,\n",
        "    attn_implementation=attn_implementation,\n",
        ").eval()\n",
        "\n",
        "# Load processor\n",
        "processor = ColQwen2Processor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-jSoTVTKHns"
      },
      "source": [
        "This notebook uses the ColQwen2 model because it has a permissive Apache 2.0 license.\n",
        "Alternatively, you can also use [ColPali](https://huggingface.co/vidore/colpali-v1.2), which has a Gemma license, or check out other available [ColVision models](https://github.com/illuin-tech/colpali). For a detailed comparison, you can also refer to [ViDoRe: The Visual Document Retrieval Benchmark](https://huggingface.co/spaces/vidore/vidore-leaderboard)\n",
        "\n",
        "If you want to use ColPali instead of ColQwen2, you can comment out the above code cell and uncomment the code cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9JQeFVH3KxCe"
      },
      "outputs": [],
      "source": [
        "#model_name = \"vidore/colpali-v1.2\"\n",
        "\n",
        "# Load model\n",
        "#colpali_model = ColPali.from_pretrained(\n",
        "#    model_name,\n",
        "#    torch_dtype=torch.bfloat16,\n",
        "#    device_map=device,\n",
        "#    attn_implementation=attn_implementation,\n",
        "#).eval()\n",
        "\n",
        "# Load processor\n",
        "#colpali_processor = ColPaliProcessor.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPBXXz46MBW0"
      },
      "source": [
        "Before we go further, let's familiarize ourselves with the ColQwen2 model. It can create multi-vector embeddings from both images and text queries. Below you can see examples of each.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tezLx7rfq2ru",
        "outputId": "199e26c5-d3fa-4bc8-fa79-1a22533c5218"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 2.0630e-02, -8.6426e-02, -7.1289e-02,  ...,  5.1758e-02,\n",
            "          -3.0365e-03,  1.1084e-01],\n",
            "         [ 1.9409e-02, -1.0840e-01, -2.6245e-02,  ...,  7.6172e-02,\n",
            "          -4.4922e-02, -1.3965e-01],\n",
            "         [-1.7242e-03, -9.8145e-02, -1.9653e-02,  ...,  7.5684e-02,\n",
            "          -3.3936e-02, -1.2891e-01],\n",
            "         ...,\n",
            "         [ 5.0537e-02, -1.0205e-01, -8.6426e-02,  ...,  4.9561e-02,\n",
            "           3.1982e-02,  8.0078e-02],\n",
            "         [ 3.9795e-02, -1.3477e-01, -5.0537e-02,  ...,  3.8330e-02,\n",
            "          -6.1523e-02, -1.2012e-01],\n",
            "         [ 9.3384e-03, -2.2168e-01, -1.4746e-01,  ..., -8.1177e-03,\n",
            "          -5.2246e-02, -3.1128e-02]],\n",
            "\n",
            "        [[ 2.0630e-02, -8.6426e-02, -7.1289e-02,  ...,  5.1758e-02,\n",
            "          -3.0365e-03,  1.1084e-01],\n",
            "         [ 1.9409e-02, -1.0840e-01, -2.6245e-02,  ...,  7.6172e-02,\n",
            "          -4.4922e-02, -1.3965e-01],\n",
            "         [-1.7242e-03, -9.8145e-02, -1.9653e-02,  ...,  7.5684e-02,\n",
            "          -3.3936e-02, -1.2891e-01],\n",
            "         ...,\n",
            "         [ 7.2266e-02, -9.3750e-02, -7.9102e-02,  ...,  5.7373e-02,\n",
            "           1.0803e-02,  7.1777e-02],\n",
            "         [ 5.2490e-02, -1.2207e-01, -4.9072e-02,  ...,  3.2471e-02,\n",
            "          -6.4453e-02, -1.1084e-01],\n",
            "         [ 1.7480e-01, -1.8457e-01, -7.2937e-03,  ...,  6.4392e-03,\n",
            "          -1.3828e-04, -5.7617e-02]]], device='cuda:0', dtype=torch.bfloat16)\n",
            "torch.Size([2, 755, 128])\n"
          ]
        }
      ],
      "source": [
        "# Sample image inputs\n",
        "images = [\n",
        "    dataset[0][\"page_image\"],\n",
        "    dataset[1][\"page_image\"],\n",
        "]\n",
        "\n",
        "# Process the inputs\n",
        "batch_images = processor.process_images(images).to(model.device)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(**batch_images)\n",
        "\n",
        "print(query_embedding)\n",
        "print(query_embedding.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wV7CxtsZMPuj",
        "outputId": "6d5d81b2-ac44-422b-e7cd-68a0f354bfb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0000, -0.0000, -0.0000,  ..., -0.0000,  0.0000,  0.0000],\n",
            "         [ 0.0238, -0.0835, -0.0752,  ...,  0.0549,  0.0076,  0.0903],\n",
            "         ...,\n",
            "         [ 0.0559, -0.0457, -0.1118,  ..., -0.1621,  0.1758,  0.1011],\n",
            "         [ 0.0525, -0.0376, -0.1172,  ..., -0.1572,  0.1787,  0.0938],\n",
            "         [ 0.0486, -0.0294, -0.1250,  ..., -0.1494,  0.1797,  0.0918]],\n",
            "\n",
            "        [[ 0.0238, -0.0835, -0.0752,  ...,  0.0549,  0.0076,  0.0903],\n",
            "         [-0.0086, -0.1021, -0.0198,  ...,  0.0708, -0.0310, -0.1367],\n",
            "         [-0.0864, -0.1230, -0.0222,  ...,  0.0776,  0.1040, -0.0128],\n",
            "         ...,\n",
            "         [-0.0544,  0.0310, -0.1318,  ..., -0.2236, -0.1445,  0.0381],\n",
            "         [-0.0679,  0.0292, -0.1484,  ..., -0.2178, -0.1387,  0.0439],\n",
            "         [-0.0742,  0.0291, -0.1553,  ..., -0.2109, -0.1289,  0.0452]]],\n",
            "       device='cuda:0', dtype=torch.bfloat16)\n",
            "torch.Size([2, 22, 128])\n"
          ]
        }
      ],
      "source": [
        "# Sample query inputs\n",
        "queries = [\n",
        "    \"A table with LLM benchmark results.\",\n",
        "    \"A figure detailing the architecture of a neural network.\",\n",
        "]\n",
        "\n",
        "# Process the inputs\n",
        "batch_queries = processor.process_queries(queries).to(model.device)\n",
        "\n",
        "# Forward pass\n",
        "with torch.no_grad():\n",
        "    query_embedding = model(**batch_queries)\n",
        "\n",
        "print(query_embedding)\n",
        "print(query_embedding.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_GEyplrgChIY"
      },
      "source": [
        "Let's write a class to wrap the multimodal late-interaction model and its embedding functionalities for convenience.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ptkqkn8-BSy"
      },
      "outputs": [],
      "source": [
        "# A convenience class to wrap the embedding functionality \n",
        "# of ColVision models like ColPali and ColQwen2 \n",
        "class ColVision:\n",
        "    def __init__(self, model, processor):\n",
        "        \"\"\"Initialize with a loaded model and processor.\"\"\"\n",
        "        self.model = model\n",
        "        self.processor = processor\n",
        "\n",
        "    # A batch size of one appears to be most performant when running on an M4.\n",
        "    # Note: Reducing the image resolution speeds up the vectorizer and produces\n",
        "    # fewer multi-vectors.\n",
        "    def multi_vectorize_image(self, img):\n",
        "        \"\"\"Return the multi-vector image of the supplied PIL image.\"\"\"\n",
        "        image_batch = self.processor.process_images([img]).to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            image_embedding = self.model(**image_batch)\n",
        "        return image_embedding[0]\n",
        "\n",
        "    def multi_vectorize_text(self, query):\n",
        "        \"\"\"Return the multi-vector embedding of the query text string.\"\"\"\n",
        "        query_batch = self.processor.process_queries([query]).to(self.model.device)\n",
        "        with torch.no_grad():\n",
        "            query_embedding = self.model(**query_batch)\n",
        "        return query_embedding[0]\n",
        "\n",
        "# Instantiate the model to be used below.\n",
        "colvision_embedder = ColVision(model, processor) # This will be instantiated after loading the model and processor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4LJSCqu7dSb"
      },
      "source": [
        "Let's verify that the embedding of images and queries works as intended.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNQwJpDVTmJ3",
        "outputId": "e7e8b886-456d-4eb6-94b9-2c1d7722e6c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([755, 128])\n",
            "torch.Size([20, 128])\n"
          ]
        }
      ],
      "source": [
        "# Sample image inputs\n",
        "images = dataset[0][\"page_image\"]\n",
        "\n",
        "page_embedding = colvision_embedder.multi_vectorize_image(images)\n",
        "print(page_embedding.shape)  # torch.Size([755, 128])\n",
        "\n",
        "queries = [\n",
        "    \"A table with LLM benchmark results.\",\n",
        "    \"A figure detailing the architecture of a neural network.\",\n",
        "]\n",
        "\n",
        "query_embeddings = [colvision_embedder.multi_vectorize_text(q) for q in queries]\n",
        "print(query_embeddings[0].shape)  # torch.Size([20, 128])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYtykMt5JuU1"
      },
      "source": [
        "## Step 4: Connect to a Weaviate vector database instance\n",
        "\n",
        "Now, you will need to connect to a running Weaviate vector database cluster.\n",
        "\n",
        "You can choose one of the following options:\n",
        "\n",
        "1. **Option 1:** You can create a 14-day free sandbox on the managed service [Weaviate Cloud (WCD)](https://console.weaviate.cloud/)\n",
        "2. **Option 2:** [Embedded Weaviate](https://docs.weaviate.io/deploy/installation-guides/embedded)\n",
        "3. **Option 3:** [Local deployment](https://docs.weaviate.io/deploy/installation-guides/docker-installation)\n",
        "4. [Other options](https://docs.weaviate.io/deploy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olDiHoMdeYvr",
        "outputId": "bf39ceba-d6fb-4f09-969e-e8e96b8c12d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Option 1: Weaviate Cloud\n",
        "WCD_URL = os.environ[\"WEAVIATE_URL\"] # Replace with your Weaviate cluster URL\n",
        "WCD_AUTH_KEY = os.environ[\"WEAVIATE_API_KEY\"] # Replace with your cluster auth key\n",
        "\n",
        "# Uncomment if you are working in a Google Colab environment\n",
        "#WCD_URL = userdata.get(\"WEAVIATE_URL\")\n",
        "#WCD_AUTH_KEY = userdata.get(\"WEAVIATE_API_KEY\")\n",
        "\n",
        "# Weaviate Cloud Deployment\n",
        "client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=WCD_URL,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(WCD_AUTH_KEY),\n",
        ")\n",
        "\n",
        "# Option 2: Embedded Weaviate instance\n",
        "# use if you want to explore Weaviate without any additional setup\n",
        "#client = weaviate.connect_to_embedded()\n",
        "\n",
        "# Option 3: Locally hosted instance of Weaviate via Docker or Kubernetes\n",
        "#!docker run --detach -p 8080:8080 -p 50051:50051 cr.weaviate.io/semitechnologies/weaviate:1.29.0\n",
        "#client = weaviate.connect_to_local()\n",
        "\n",
        "print(client.is_ready())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H15lTeNmD3bK"
      },
      "source": [
        "For this tutorial, you will need the Weaviate `v1.29.0` or higher.\n",
        "Let's make sure we have the required version:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "qxB3jcGDD-RA",
        "outputId": "a729d1be-b59b-4796-eced-600f68ab7a79"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'1.32.4'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "client.get_meta()['version']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZmkYAnQJ_FL"
      },
      "source": [
        "## Step 5: Create a collection\n",
        "\n",
        "Next, we will create a collection that will hold the embeddings of the images of the PDF document pages.\n",
        "\n",
        "We will not define a built-in vectorizer but use the [Bring Your Own Vectors (BYOV) approach](https://docs.weaviate.io/weaviate/starter-guides/custom-vectors), where we manually embed queries and PDF documents at ingestions and query stage.\n",
        "\n",
        "Additionally, if you are interested in using the [MUVERA encoding algorithm](https://weaviate.io/blog/muvera) for multi-vector embeddings, you can uncomment it in the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i_l3EJviknzC"
      },
      "outputs": [],
      "source": [
        "collection_name = \"PDFDocuments\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BoypH0Tv-BS0"
      },
      "outputs": [],
      "source": [
        "# Delete the collection if it already exists\n",
        "# Note: in practice, you shouldn't rerun this cell, as it deletes your data\n",
        "# in \"PDFDocuments\", and then you need to re-import it again.\n",
        "#if client.collections.exists(collection_name):\n",
        "#  client.collections.delete(collection_name)\n",
        "\n",
        "# Create a collection\n",
        "collection = client.collections.create(\n",
        "    name=collection_name,\n",
        "    properties=[\n",
        "        wc.Property(name=\"page_id\", data_type=wc.DataType.INT),\n",
        "        wc.Property(name=\"dataset_index\", data_type=wc.DataType.INT),\n",
        "        wc.Property(name=\"paper_title\", data_type=wc.DataType.TEXT),\n",
        "        wc.Property(name=\"paper_arxiv_id\", data_type=wc.DataType.TEXT),\n",
        "        wc.Property(name=\"page_number\", data_type=wc.DataType.INT),\n",
        "    ],\n",
        "    vector_config=[\n",
        "        Configure.MultiVectors.self_provided(\n",
        "            name=\"colqwen\",\n",
        "            #encoding=Configure.VectorIndex.MultiVector.Encoding.muvera(),\n",
        "            vector_index_config=Configure.VectorIndex.hnsw(\n",
        "                multi_vector=Configure.VectorIndex.MultiVector.multi_vector()\n",
        "            )\n",
        "    )]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hbpVPUpMk4jJ"
      },
      "source": [
        "## Step 6: Uploading the vectors to Weaviate\n",
        "\n",
        "In this step, we're indexing the vectors into our Weaviate Collection in batches.\n",
        "\n",
        "For each batch, the images are processed and encoded using the ColPali model, turning them into multi-vector embeddings.\n",
        "These embeddings are then converted from tensors into lists of vectors, capturing key details from each image and creating a multi-vector representation for each document.\n",
        "This setup works well with Weaviate's multivector capabilities.\n",
        "\n",
        "After processing, the vectors and any metadata are uploaded to Weaviate, gradually building up the index.\n",
        "You can lower or increase the `batch_size` depending on your available GPU resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s234AW0u-BS0",
        "outputId": "62e4aaae-86e3-4bdc-bacb-1a246ad35da2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 1/399 Page objects to Weaviate.\n",
            "Added 26/399 Page objects to Weaviate.\n",
            "Added 51/399 Page objects to Weaviate.\n",
            "Added 76/399 Page objects to Weaviate.\n",
            "Added 101/399 Page objects to Weaviate.\n",
            "Added 126/399 Page objects to Weaviate.\n",
            "Added 151/399 Page objects to Weaviate.\n",
            "Added 176/399 Page objects to Weaviate.\n",
            "Added 201/399 Page objects to Weaviate.\n",
            "Added 226/399 Page objects to Weaviate.\n",
            "Added 251/399 Page objects to Weaviate.\n",
            "Added 276/399 Page objects to Weaviate.\n",
            "Added 301/399 Page objects to Weaviate.\n",
            "Added 326/399 Page objects to Weaviate.\n",
            "Added 351/399 Page objects to Weaviate.\n",
            "Added 376/399 Page objects to Weaviate.\n"
          ]
        }
      ],
      "source": [
        "# Map of page ids to images to support displaying the image corresponding to a\n",
        "# particular page id.\n",
        "page_images = {}\n",
        "\n",
        "with collection.batch.dynamic() as batch:\n",
        "    for i in range(len(dataset)):\n",
        "        p = dataset[i]\n",
        "        page_images[p[\"page_id\"]] = p[\"page_image\"]\n",
        "\n",
        "        batch.add_object(\n",
        "            properties={\n",
        "                \"page_id\": p[\"page_id\"],\n",
        "                \"paper_title\": p[\"paper_title\"],\n",
        "                \"paper_arxiv_id\": p[\"paper_arxiv_id\"],\n",
        "                \"page_number\": p[\"page_number\"],\n",
        "                },\n",
        "            vector={\"colqwen\": colvision_embedder.multi_vectorize_image(p[\"page_image\"]).cpu().float().numpy().tolist()})\n",
        "\n",
        "        if i % 25 == 0:\n",
        "            print(f\"Added {i+1}/{len(dataset)} Page objects to Weaviate.\")\n",
        "\n",
        "    batch.flush()\n",
        "\n",
        "# Delete dataset after creating page_images dict to hold the images\n",
        "del dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgQGaZemnvsA",
        "outputId": "8815007b-dc80-421d-93ab-f5e5642bb6d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "399"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(collection)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nznNtelLAZ7t"
      },
      "source": [
        "## Step 7: Multimodal Retrieval Query\n",
        "\n",
        "As an example of what we are going to build, consider the following actual demo query and resulting PDF page from our collection (nearest neighbor):\n",
        "\n",
        "- Query: \"How does DeepSeek-V2 compare against the LLaMA family of LLMs?\"\n",
        "- Nearest neighbor:  \"DeepSeek-V2: A Strong Economical and Efficient Mixture-of-Experts Language Model\" (arXiv: 2405.04434), Page: 1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "focgDIfayLDT"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'Python 3.13.5' requires the ipykernel package.\n",
            "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: '/opt/homebrew/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "query = \"How does DeepSeek-V2 compare against the LLaMA family of LLMs?\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11ZdCU3YyPWq"
      },
      "source": [
        "By inspecting the first page of the [DeepSeek-V2 paper](https://arxiv.org/abs/2405.04434), we see that it does indeed contain a figure that is relevant for answering our query:\n",
        "\n",
        "<img src=\"https://github.com/weaviate/recipes/blob/main/weaviate-features/multi-vector/figures/deepseek_efficiency.jpeg?raw=1\" width=\"700px\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPyYupBB6k-4"
      },
      "source": [
        "Note: To avoid `OutOfMemoryError` on freely available resources like Google Colab, we will only retrieve a single document. If you have resources with more memory available, you can set the `limit`parameter to a higher value, like e.g., `limit=3` to increase the number of retrieved PDF pages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q25GcAdO-BS0",
        "outputId": "cb0e3789-c842-408a-a23a-da94f0601211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The most relevant documents for the query \"How does DeepSeek-V2 compare against the LLaMA family of LLMs?\" by order of relevance:\n",
            "\n",
            "1) MaxSim: 23.12, Title: \"DeepSeek-V2: A Strong Economical and Efficient Mixture-of-Experts Language Model\" (arXiv: 2405.04434), Page: 1\n"
          ]
        }
      ],
      "source": [
        "response = collection.query.near_vector(\n",
        "    near_vector=colvision_embedder.multi_vectorize_text(query).cpu().float().numpy(),\n",
        "    target_vector=\"colqwen\",\n",
        "    limit=1,\n",
        "    return_metadata=MetadataQuery(distance=True), # Needed to return MaxSim score\n",
        ")\n",
        "\n",
        "print(f\"The most relevant documents for the query \\\"{query}\\\" by order of relevance:\\n\")\n",
        "result_images = []\n",
        "for i, o in enumerate(response.objects):\n",
        "    p = o.properties\n",
        "    print(\n",
        "        f\"{i+1}) MaxSim: {-o.metadata.distance:.2f}, \"\n",
        "        + f\"Title: \\\"{p['paper_title']}\\\" \"\n",
        "        + f\"(arXiv: {p['paper_arxiv_id']}), \"\n",
        "        + f\"Page: {int(p['page_number'])}\"\n",
        "    )\n",
        "    result_images.append(page_images[p[\"page_id\"]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3naCOvSFyXdA"
      },
      "source": [
        "The retrieved page with the highest MaxSim score is indeed the page with the figure we mentioned earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sAWd78W0th3B",
        "outputId": "fc7015ca-863a-4707-9466-e1dd3d404763"
      },
      "outputs": [],
      "source": [
        "closest_page_id = response.objects[0].properties['page_id']\n",
        "image = page_images[closest_page_id]\n",
        "display(image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hKY4Jk3xcaa"
      },
      "source": [
        "![Retrieved page](./figures/retrieved_page.png)\n",
        "\n",
        "Let's visualize the similarity maps for the retrieved PDF document page to see the semantic similarity between each token in the user query and the image patches. This is an optional step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuLKgoqMtqbQ",
        "outputId": "82b18193-2e32-4509-f0e8-432bdc29a490"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocess inputs\n",
        "batch_images = processor.process_images([image]).to(device)\n",
        "batch_queries = processor.process_queries([query]).to(device)\n",
        "\n",
        "# Forward passes\n",
        "with torch.no_grad():\n",
        "    image_embeddings = model.forward(**batch_images)\n",
        "    query_embeddings = model.forward(**batch_queries)\n",
        "\n",
        "# Get the number of image patches\n",
        "n_patches = processor.get_n_patches(\n",
        "    image_size=image.size,\n",
        "    spatial_merge_size=model.spatial_merge_size,\n",
        ")\n",
        "\n",
        "# Get the tensor mask to filter out the embeddings that are not related to the image\n",
        "image_mask = processor.get_image_mask(batch_images)\n",
        "\n",
        "# Generate the similarity maps\n",
        "batched_similarity_maps = get_similarity_maps_from_embeddings(\n",
        "    image_embeddings=image_embeddings,\n",
        "    query_embeddings=query_embeddings,\n",
        "    n_patches=n_patches,\n",
        "    image_mask=image_mask,\n",
        ")\n",
        "\n",
        "# Get the similarity map for our (only) input image\n",
        "similarity_maps = batched_similarity_maps[0]  # (query_length, n_patches_x, n_patches_y)\n",
        "\n",
        "print(f\"Similarity map shape: (query_length, n_patches_x, n_patches_y) = {tuple(similarity_maps.shape)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OqeiIdyDvYYy",
        "outputId": "195ee5f6-c348-449b-8da9-976ff35e6b0b"
      },
      "outputs": [],
      "source": [
        "# Remove the padding tokens and the query augmentation tokens\n",
        "query_content = processor.decode(batch_queries.input_ids[0])\n",
        "query_content = query_content.replace(processor.tokenizer.pad_token, \"\")\n",
        "query_content = query_content.replace(processor.query_augmentation_token, \"\").strip()\n",
        "\n",
        "# Retokenize the cleaned query\n",
        "query_tokens = processor.tokenizer.tokenize(query_content)\n",
        "\n",
        "# Use this cell output to choose a token using its index\n",
        "for idex, val in enumerate(query_tokens):\n",
        "    print(f\"{idex}: {val}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's check the similarity plot for the token \"MA\" in \"LLaMA\". (Note that similarity maps are created for each token separately.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ByclarxFtRIL",
        "outputId": "88139ed0-bd6f-432e-bb46-3385d85ff46d"
      },
      "outputs": [],
      "source": [
        "token_idx = 13\n",
        "\n",
        "fig, ax = plot_similarity_map(\n",
        "    image=image,\n",
        "    similarity_map=similarity_maps[token_idx],\n",
        "    figsize=(18, 18),\n",
        "    show_colorbar=False,\n",
        ")\n",
        "\n",
        "max_sim_score = similarity_maps[token_idx, :, :].max().item()\n",
        "ax.set_title(f\"Token #{token_idx}: `{query_tokens[token_idx]}`. MaxSim score: {max_sim_score:.2f}\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Similarity map](./figures/similarity_map.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIfTgjfzqSf7"
      },
      "outputs": [],
      "source": [
        "# Delete variables used for visualization\n",
        "del batched_similarity_maps, similarity_maps, n_patches, query_content, query_tokens, token_idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9v9BhSXAdUb"
      },
      "source": [
        "## Step 8: Extension to Multimodal RAG using Qwen2.5\n",
        "\n",
        "The above example gives us the most relevant pages to begin looking at to answer our query. Let's extend this multimodal document retrieval pipeline to a multimodal RAG pipeline.\n",
        "\n",
        "Vision language models (VLMs) are Large Language Models with vision capabilities. They are now powerful enough that we can give the query and relevant pages to such a model and have it produce an answer to our query in plain text.\n",
        "\n",
        "To accomplish this we are going to feed the top results into the\n",
        "state-of-the-art VLM [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 470,
          "referenced_widgets": [
            "7d96e9f25ed04317b7fc188a68b641b3",
            "579d05e732ee4123890d5c1ac7327d2b",
            "ea5b8eb4509648f983df65e25105f312",
            "b2465e6953a54d5d8ee3ef7f0d59c9fd",
            "a78a659b1ed74ee49b8ba0450d4686dd",
            "3902db91aa6948098ee595ab91e8ed36",
            "82a17d03c1f34af78c541d57c8125e0f",
            "4e6bee8927514d0cab30411d7f080848",
            "ae970d2245ed4c66960e0172a1c2fbc9",
            "15d57c722a344154bce9d28230a5f61e",
            "f8a275ac24954cd480df6778349fd224",
            "781a680b4a7740909710bb2d916e6c55",
            "31147e5f74034e1392bd63f2580a4516",
            "38ec18aba994448597fb3f9059e0baba",
            "dfb60a6c5cba4f959a3961dd7e51056e",
            "8a57035f7b9644faa45987cf29b1c262",
            "70bacb1ef2c346ccb085c38f51b3bf30",
            "84712401317f434eb0ae8f23474deccb",
            "2d019778731f4b52a992caab57f07d49",
            "ee377dd45e4b433bbd85ac6687e449f9",
            "eee52b210090489482a6f7812468f68d",
            "0f8ee6a7fcff4590a834a143f2848014",
            "4098284812d4474fbea89454dbc7be4e",
            "139aff8c399f4d138819574d13c1eb8e",
            "18c4c6a2c19045569b00d02240dca01b",
            "4f6eccad166943d9b59338c8b6d885a1",
            "80c57e1aea5d40ebad0025670f41816e",
            "3d8a90d5c9f143ef9ea588f176745eb5",
            "3f0b6d7294c44575a3deaa78cd991269",
            "5f74d19f4d4c4a0bae3fe4fa76bcb532",
            "b5bf5c12da894b329618060d179497fa",
            "8c40b7388fd14607bf435bf81061e4f5",
            "74b7a67d9bad4e9a82f86a3ee7feb5c2",
            "e7db9740c860488badac58f4be66be90",
            "d9bce793d33f460cb8f136b3054b8705",
            "2ccd36fd39464ae2a9e5d22cbade576c",
            "f13029fe62824989b5bb8c5a458bd590",
            "a85d9e9c50b24d17a6045fbce267fe67",
            "99b3bc4f648c4827a82d5b8f7a7f11d3",
            "3a84b118d0624005a5b2dac57bae7533",
            "37c363f3120645139b495c97c5a80cb4",
            "4eb1d0d276bb41628949abcfa1f0e14f",
            "b5976bfdee7d475dbfac2f358eb7c509",
            "35db3e1165664ad8a44417c7eb986704",
            "b4972b896e5c43e9a043ec123f15044d",
            "2e7b89acd74543299b86209088d1580c",
            "b2c451d7aa7141b49b7e1b07a67f1e4f",
            "c9988cccabc049cfb6e2db34ddf5b68e",
            "2082dad312904f018a0ffce427e5576f",
            "a5fe9eddd4624359b5170267fc88275c",
            "68dbfb128b734c9fbaa06ec24d850580",
            "049989fbd15c49c88a7d030fc899c3bc",
            "8dd1ae8ab0de443ba750c5764e58e3bc",
            "c45aa59d3ac842c49418babc866c4f79",
            "2a54f4a2f27444dcb21a03881677ed40",
            "203d4acc49fe486fa0774acdeadaa39e",
            "f0abedf704d44d6688c05918d8b3e728",
            "da3c57e1747f445fbe8aaf4584edb43a",
            "7556f6468297467889238c0668a9e698",
            "be3f421b459c4932be76dbd86764f84e",
            "090e45bbd5ca4196af29ed9d30f9f9c2",
            "865b60810897406cb2f70a05a5213358",
            "753f18d4a9224b88ad659ef1ee707837",
            "b63056ec786944b79301649740369dd0",
            "f7040454f88f45eba79bbc2b8acc303a",
            "3ed3797fa6e2404da4e9c311611da497",
            "ad7dcf40f3a841e68ded277c9c09099a",
            "cb968270faa24dcca63b34920d95ee8d",
            "55c93b6a316a4ee798fc1b20a0b76bda",
            "8e3c50169b30466cad5b4904cff9c3b5",
            "ffc9cc1d58154ee2a3630e89d7648d63",
            "1bb4df76527745ecacc75ada2557d885",
            "3c29a9483b9c418ba29a2378e4073a89",
            "72da0259626b42b49cf9b6839d381e2a",
            "3f1d739e5b3747f6898d73b1d3ee1ac3",
            "ca9747bf5cc74f69b44ee289dd6798e9",
            "5bad3f3fa74a424291d419f9eb8a1f86",
            "ee5f8382b38c40148197eed4464129be",
            "233abd2c9eab4df89cbcdb9369f94907",
            "93c3f6479ae04687bd89ab7e3fec3691",
            "8710f1d4d90541bfa105557575442055",
            "1def1e5c77a14511bb3f9405a041cced",
            "1b9bbb276850417abfbad9f72aa6e0e9",
            "94233ef30b9e4b2685032fb33ecc6aee",
            "cdbdc268425648eebdd415b21dc0a6a5",
            "e832731fe8ef4984a697d956e80d3655",
            "dda03ff85422480abd04ded070aa70bc",
            "21165928f0404e95a92f24aa64deb830",
            "37f9fde6da554a37ba60334b09b38b0a",
            "a157b51bf75a48bdb6efde0fa58e6b83",
            "b7f43ba035de4e919fff7e3fffb69d80",
            "df5149d81885429dae6e279f37060afd",
            "22eb3c58639c436bb0708629e486ea6a",
            "08648c3d2998487ea2b69cbd3ed399dd",
            "96b64afa0628455b91d5092a487d861e",
            "bf06c85453d64468b528e420f33d1952",
            "0e9e4a41603e4f7585159771222c53df",
            "072fb5172b5646239f3d28aadcf34ec5",
            "f53c834c683549449c6a1a16ba6bd962",
            "6d10953e39774d2887724171107f0ba0",
            "d4a09c7e3c024c0495808664529da4c6",
            "4e10b9621aae4231840156233e911858",
            "c9705a409589481192ed5975dc5ecef6",
            "6b18ea18e19a4e8699f0c494d5b7975f",
            "d5757fbc21944ca2836a6ea61802b6dd",
            "c3c2bdd63c5643ea9d99f391160ad76f",
            "1c71369d95614f5cbe75e9a85e1f5967",
            "a8b3de1f08804e998aba8cc87b2bf644",
            "33a4164eec7540198dd58b0f3694f4f3",
            "884f3c3dccc0478d8d472dfcb8bddf4a",
            "4643c9d643134a4ebbfbb334c8f2085f",
            "240ccb1d42a44330bd432a5c0da97cef",
            "12bbf4be13874494a1787978e9154331",
            "12d62b610a22422eb3470cbb2d6bcf76",
            "e15dcfe5f309428dbd068c2024026591",
            "28f36f57e24c4978983f1c57c5c4439d",
            "64129fd2ba4141dc967ac8bf5d63c010",
            "7ebf0344eb694529a73273abd6a717d4",
            "66e6a90d15494a05b52271bc5766a877",
            "3e2575dcda884ffeb1523a141cd82d42",
            "ddd805c81e0749fdb6a61c9eaf78d455",
            "9103c313fb21423a9a36a8b2a10f2a8c",
            "215ec294cc744ee18c454f5e7e99d973",
            "c22ab4ecd40a4f73a564a7870062bf87",
            "b1a4994891f146c2bf9ebe4da30a81af",
            "30af6b51811d4be49844709036d36a1f",
            "d0bbdf11bca44a69b67784170b5d147f",
            "30890dd8481b493d8742fc8a13f6bc60",
            "4c41e47e584f41108d49609bed1029a6",
            "856ae6e229644146817adf3a88396500",
            "643c64fdf10645e7a11452b137574567",
            "1235639bbde34e24961e5f29026aee2c",
            "ce601ac26a2644e18903eef18af99e77",
            "7c642a5bee88417ea64007917fb1432a",
            "4b66d1b7e0c14e0b97a4319df6d9e91b",
            "45c611a489c64916b582cfe8722ee2f7",
            "9653a3a1bfa041c4abee311475649a3a",
            "ee280d44256b4475926183c6cc5d9d8c",
            "68f7d0dd8f994e60b99c2b84b615ea71",
            "cbeb97f357414f94894db76e955a83b2",
            "52eb5f5b3ad54fccacc9ba265fbc5b02",
            "428fc533eb2d4b5783f54d5159baa717",
            "4fab4433289b44eb8e1425a62e9fd4d4"
          ]
        },
        "id": "0dn31J9L-BS1",
        "outputId": "07fdd2de-d162-4523-90a5-337eab48cdac"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7d96e9f25ed04317b7fc188a68b641b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "781a680b4a7740909710bb2d916e6c55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4098284812d4474fbea89454dbc7be4e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7db9740c860488badac58f4be66be90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00002-of-00002.safetensors:   0%|          | 0.00/3.53G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4972b896e5c43e9a043ec123f15044d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00002.safetensors:   0%|          | 0.00/3.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "203d4acc49fe486fa0774acdeadaa39e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ad7dcf40f3a841e68ded277c9c09099a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/216 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ee5f8382b38c40148197eed4464129be",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37f9fde6da554a37ba60334b09b38b0a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6d10953e39774d2887724171107f0ba0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4643c9d643134a4ebbfbb334c8f2085f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9103c313fb21423a9a36a8b2a10f2a8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce601ac26a2644e18903eef18af99e77",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "chat_template.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Setting up Qwen2.5-VL-3B-Instruct for generating answers from a query string\n",
        "# plus a collection of (images of) PDF pages.\n",
        "\n",
        "class QwenVL:\n",
        "    def __init__(self):\n",
        "        # Adjust the settings to your available architecture, see the link\n",
        "        # https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct for examples.\n",
        "        self.model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n",
        "            \"Qwen/Qwen2.5-VL-3B-Instruct\",\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=device,\n",
        "            attn_implementation=attn_implementation,\n",
        "        )\n",
        "\n",
        "        min_pixels = 256*28*28\n",
        "        max_pixels = 1280*28*28\n",
        "        self.processor = AutoProcessor.from_pretrained(\n",
        "            \"Qwen/Qwen2.5-VL-3B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n",
        "\n",
        "    def query_images(self, query, images):\n",
        "        \"\"\"Generate a textual response to the query (text) based on the information in the supplied list of PIL images.\"\"\"\n",
        "        # Preparation for inference.\n",
        "        # Convert the images to base64 strings.\n",
        "        content = []\n",
        "\n",
        "        for img in images:\n",
        "            buffer = BytesIO()\n",
        "            img.save(buffer, format=\"jpeg\")\n",
        "            img_base64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
        "            content.append({\"type\": \"image\", \"image\": f\"data:image;base64,{img_base64}\"})\n",
        "\n",
        "        content.append({\"type\": \"text\", \"text\": query})\n",
        "        messages = [{\"role\": \"user\", \"content\": content}]\n",
        "\n",
        "        text = self.processor.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "        inputs = self.processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Inference: Generation of the output.\n",
        "        generated_ids = self.model.generate(**inputs, max_new_tokens=128)\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "        return self.processor.batch_decode(\n",
        "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "        )[0]\n",
        "\n",
        "# Instantiate the model to be used below.\n",
        "qwenvl = QwenVL()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7JFiOGJVDF1"
      },
      "source": [
        "The response from `Qwen2.5-VL-3B-Instruct` based on the retrieved PDF pages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ZoJUbdK4UMdh",
        "outputId": "56d93825-12a2-4762-913f-812b9b2bad8a"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DeepSeek-V2 achieves significantly stronger performance than the LLaMA family of LLMs, while also saving 42.5% of training costs and boosting the maximum generation throughput to 5.76 times.'"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "qwenvl.query_images(query, result_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As you can see, the multimodal RAG pipeline was able to answer the original query: \"How does DeepSeek-V2 compare against the LLaMA family of LLMs?\". For this, the ColQwen2 retrieval model retrieved the correct PDF page from the \n",
        "\"DeepSeek-V2: A Strong Economical and Efficient Mixture-of-Experts Language Model\" paper and used both the text and visual from the retrieved PDF page to answer the question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HgQBQPK3See"
      },
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrates a multimodal RAG pipeline over PDF documents using ColQwen2 for multi-vector embeddings, a Weaviate vector database for storage and retrieval, and Qwen2.5-VL-3B-Instruct for generating answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir2zXq3XHIDf"
      },
      "source": [
        "## References\n",
        "\n",
        "- Faysse, M., Sibille, H., Wu, T., Omrani, B., Viaud, G., Hudelot, C., Colombo, P. (2024). ColPali: Efficient Document Retrieval with Vision Language Models. arXiv. https://doi.org/10.48550/arXiv.2407.01449\n",
        "- [ColPali GitHub repository](https://github.com/illuin-tech/colpali)\n",
        "- [ColPali Cookbook](https://github.com/tonywu71/colpali-cookbooks)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
