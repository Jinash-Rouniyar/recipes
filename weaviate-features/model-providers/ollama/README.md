The Ollama library allows you to easily run a wide range of models on your own device. You can use Weaviate's [Ollama integration](https://docs.weaviate.io/weaviate/model-providers/ollama) to use a variety of models. Below, you can find a list of Notebooks.

| Model | Task | Notebook|
--------------------------
| `deepseek-r1:1.5b` | RAG | (Notebook)[https://github.com/weaviate/recipes/weaviate-features/model-providers/deepseek/rag_deepseek_r1:1.5b.ipynb] |
| `all-minilm` and `llama2` | RAG | (Notebook)[/Users/leonie/Documents/code/recipes/weaviate-features/model-providers/meta/rag_llama_2_ollama.ipynb] |
| `all-minilm` and `llama3` | RAG | (Notebook)[/Users/leonie/Documents/code/recipes/weaviate-features/model-providers/meta/rag_llama_3_ollama.ipynb] |