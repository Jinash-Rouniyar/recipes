{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G9K6KdPWgS87"
      },
      "source": [
        "## Cardinal ↔ Weaviate RAG Demo\n",
        "\n",
        "We will:\n",
        "- Pull PDFs from S3 (or use URLs)\n",
        "- Send each file to Cardinal /rag\n",
        "- Convert Cardinal’s inch-based boxes → points → normalized percentages\n",
        "- Upsert chunks to Weaviate\n",
        "- Run aggregate, hybrid, vector, and generative queries\n",
        "- (Optional) Draw overlay boxes on page images for quick visual QA\n",
        "\n",
        "Requires: Cardinal API key, Weaviate (Cloud/Embedded/Local), AWS creds (if using S3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcWZvE7Cq83i"
      },
      "source": [
        "## 0) Install deps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKzNstZigLkp",
        "outputId": "e0fc89c4-1bb3-40f8-a899-4af4186d1187"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/325.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.6/325.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.7/325.7 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/40.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.0/322.0 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.0 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 6.32.1 which is incompatible.\n",
            "tensorflow 2.19.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.32.1 which is incompatible.\n",
            "mcp 1.13.1 requires httpx>=0.27.1, but you have httpx 0.27.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.32.1 which is incompatible.\n",
            "google-genai 1.33.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "\n",
        "!pip install -q \"weaviate-client==4.6.5\" python-dotenv boto3 requests tqdm pandas pillow PyMuPDF pdf2image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDBNPnzcrAqK"
      },
      "source": [
        "## 1) Load environment variables\n",
        "\n",
        "## 2) Connect to Weaviate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tKTrKyO_isDX",
        "outputId": "9b47ea8d-ae7d-481d-ef73-cf255a38a628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os, dotenv\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/.env')\n",
        "\n",
        "# Cardinal\n",
        "CARDINAL_URL = os.getenv(\"CARDINAL_URL\", \"https://api.trycardinal.ai\")\n",
        "CARDINAL_API_KEY = os.getenv(\"CARDINAL_API_KEY\")\n",
        "\n",
        "# Weaviate\n",
        "WEAVIATE_URL = os.getenv(\"WEAVIATE_URL\")\n",
        "WEAVIATE_API_KEY = os.getenv(\"WEAVIATE_API_KEY\")\n",
        "\n",
        "# OpenAI (optional; only if you want Weaviate to use OpenAI for vectorizer/generative)\n",
        "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# S3 (optional)\n",
        "AWS_ACCESS_KEY_ID = os.getenv(\"AWS_ACCESS_KEY_ID\") or os.getenv(\"AWS_KEY\")\n",
        "AWS_SECRET_ACCESS_KEY = os.getenv(\"AWS_SECRET_ACCESS_KEY\") or os.getenv(\"AWS_SECRET\")\n",
        "AWS_S3_BUCKET = os.getenv(\"AWS_S3_BUCKET\") or os.getenv(\"AWS_S3_NAME\")  # e.g. s3://my-bucket/path\n",
        "AWS_REGION = os.getenv(\"AWS_REGION\")\n",
        "\n",
        "S3_PREFIX = os.getenv(\"S3_PREFIX\", \"\")  # e.g. folder/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45mM8HqnqkWl",
        "outputId": "5ccc2810-d1eb-42d1-e504-fd4db3abab64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Weaviate client version: 4.6.5\n",
            "Connected to Weaviate: True\n"
          ]
        }
      ],
      "source": [
        "import weaviate\n",
        "import weaviate.classes.config as wc\n",
        "import os\n",
        "\n",
        "# Check Weaviate client version\n",
        "print(f\"Weaviate client version: {weaviate.__version__}\")\n",
        "\n",
        "client = weaviate.connect_to_wcs(\n",
        "    cluster_url=WEAVIATE_URL,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(WEAVIATE_API_KEY),\n",
        "    headers={\"X-OpenAI-Api-Key\": OPENAI_API_KEY} if OPENAI_API_KEY else {}\n",
        ")\n",
        "print(\"Connected to Weaviate:\", client.is_ready())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lz3TvTwvrGQi"
      },
      "source": [
        "## 3) Create a Weaviate collection (CardinalDemo)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p7IqE-xrqvCb",
        "outputId": "a443df61-b990-4993-ecdf-c836e0ad6f78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Deleted existing CardinalDemo collection\n",
            "Created CardinalDemo collection\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    client.collections.delete(\"CardinalDemo\")\n",
        "    print(\"Deleted existing CardinalDemo collection\")\n",
        "except Exception as e:\n",
        "    print(f\"No existing collection to delete: {e}\")\n",
        "\n",
        "# STEP 3: Create new collection\n",
        "documents = client.collections.create(\n",
        "    name=\"CardinalDemo\",\n",
        "    vectorizer_config=wc.Configure.Vectorizer.text2vec_openai(\n",
        "        model=\"text-embedding-3-small\",\n",
        "        type_=\"text\"\n",
        "    ),\n",
        "    generative_config=wc.Configure.Generative.openai(\n",
        "        model=\"gpt-4\"\n",
        "    ),\n",
        "    properties=[\n",
        "        wc.Property(name=\"text\", data_type=wc.DataType.TEXT),\n",
        "        wc.Property(name=\"type\", data_type=wc.DataType.TEXT),\n",
        "        wc.Property(name=\"element_id\", data_type=wc.DataType.TEXT, skip_vectorization=True),\n",
        "        wc.Property(name=\"page_number\", data_type=wc.DataType.INT),\n",
        "        wc.Property(name=\"page_width_pts\", data_type=wc.DataType.NUMBER, skip_vectorization=True),\n",
        "        wc.Property(name=\"page_height_pts\", data_type=wc.DataType.NUMBER, skip_vectorization=True),\n",
        "\n",
        "        # Original bbox (inches)\n",
        "        wc.Property(name=\"bbox_in\", data_type=wc.DataType.OBJECT, nested_properties=[\n",
        "            wc.Property(name=\"min_x\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"min_y\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"max_x\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"max_y\", data_type=wc.DataType.NUMBER),\n",
        "        ], skip_vectorization=True),\n",
        "\n",
        "        # Derived bbox (points)\n",
        "        wc.Property(name=\"bbox_pts\", data_type=wc.DataType.OBJECT, nested_properties=[\n",
        "            wc.Property(name=\"x\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"y\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"w\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"h\", data_type=wc.DataType.NUMBER),\n",
        "        ], skip_vectorization=True),\n",
        "\n",
        "        # Normalized bbox (%)\n",
        "        wc.Property(name=\"bbox_norm\", data_type=wc.DataType.OBJECT, nested_properties=[\n",
        "            wc.Property(name=\"left\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"top\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"width\", data_type=wc.DataType.NUMBER),\n",
        "            wc.Property(name=\"height\", data_type=wc.DataType.NUMBER),\n",
        "        ], skip_vectorization=True),\n",
        "\n",
        "        # File metadata\n",
        "        wc.Property(name=\"filename\", data_type=wc.DataType.TEXT),\n",
        "        wc.Property(name=\"filetype\", data_type=wc.DataType.TEXT, skip_vectorization=True),\n",
        "        wc.Property(name=\"languages\", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True),\n",
        "        wc.Property(name=\"source_url\", data_type=wc.DataType.TEXT, skip_vectorization=True),\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(\"Created CardinalDemo collection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_vY3vrtWrJH0"
      },
      "source": [
        "## 4) Import helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vonVDI-aqzhN"
      },
      "outputs": [],
      "source": [
        "import json, requests, re\n",
        "from typing import Dict, Any, List, Optional\n",
        "from urllib.parse import urlparse, quote\n",
        "from tqdm import tqdm\n",
        "\n",
        "INCHES_TO_POINTS = 72.0\n",
        "\n",
        "def list_s3_urls(bucket_with_prefix: str, limit: Optional[int]=None) -> List[str]:\n",
        "    \"\"\"Return a list of s3://bucket/key URLs.\"\"\"\n",
        "    if not bucket_with_prefix or not bucket_with_prefix.startswith(\"s3://\"):\n",
        "        return []\n",
        "\n",
        "    import boto3\n",
        "    s3 = boto3.client(\n",
        "        \"s3\",\n",
        "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
        "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
        "        region_name=AWS_REGION\n",
        "    )\n",
        "    parsed = urlparse(bucket_with_prefix)\n",
        "    bucket = parsed.netloc\n",
        "    prefix = parsed.path.lstrip(\"/\")\n",
        "\n",
        "    urls = []\n",
        "    paginator = s3.get_paginator('list_objects_v2')\n",
        "    for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
        "        for obj in page.get(\"Contents\", []):\n",
        "            key = obj[\"Key\"]\n",
        "            if re.search(r\"\\.(pdf|PDF|png|jpg|jpeg)$\", key):\n",
        "                urls.append(f\"s3://{bucket}/{key}\")\n",
        "                if limit and len(urls) >= limit:\n",
        "                    return urls\n",
        "    return urls\n",
        "\n",
        "def s3_to_https(s3_url: str) -> Optional[str]:\n",
        "    \"\"\"Convert s3://bucket/key to public HTTPS URL.\"\"\"\n",
        "    if not s3_url.startswith(\"s3://\"):\n",
        "        return s3_url\n",
        "\n",
        "    parsed = urlparse(s3_url)\n",
        "    bucket = parsed.netloc\n",
        "    key = parsed.path.lstrip(\"/\")\n",
        "    encoded_key = quote(key, safe='/')\n",
        "\n",
        "    # Try region-specific format\n",
        "    return f\"https://{bucket}.s3.{AWS_REGION}.amazonaws.com/{encoded_key}\"\n",
        "\n",
        "def process_with_cardinal(file_url: str) -> Dict[str, Any]:\n",
        "    \"\"\"Call Cardinal /rag, which will return Markdown content with their bounding boxes.\"\"\"\n",
        "    url = f\"{CARDINAL_URL.rstrip('/')}/rag\"\n",
        "    form = {\n",
        "        \"fileUrl\": file_url\n",
        "    }\n",
        "    headers = {\"x-api-key\": CARDINAL_API_KEY}\n",
        "\n",
        "    print(f\"Processing: {file_url}\")\n",
        "    r = requests.post(url, data=form, headers=headers, timeout=180)\n",
        "    r.raise_for_status()\n",
        "    return r.json()\n",
        "\n",
        "def bbox_in_to_pts(bbox_in: Dict[str, float]) -> Dict[str, float]:\n",
        "    x = bbox_in[\"min_x\"] * INCHES_TO_POINTS\n",
        "    y = bbox_in[\"min_y\"] * INCHES_TO_POINTS\n",
        "    w = (bbox_in[\"max_x\"] - bbox_in[\"min_x\"]) * INCHES_TO_POINTS\n",
        "    h = (bbox_in[\"max_y\"] - bbox_in[\"min_y\"]) * INCHES_TO_POINTS\n",
        "    return {\"x\": x, \"y\": y, \"w\": w, \"h\": h}\n",
        "\n",
        "def bbox_pts_to_norm(bbox_pts: Dict[str, float],\n",
        "                     page_w_pts: float,\n",
        "                     page_h_pts: float) -> Dict[str, float]:\n",
        "    left = 100.0 * bbox_pts[\"x\"] / max(page_w_pts, 1e-6)\n",
        "    top = 100.0 * bbox_pts[\"y\"] / max(page_h_pts, 1e-6)\n",
        "    width = 100.0 * bbox_pts[\"w\"] / max(page_w_pts, 1e-6)\n",
        "    height = 100.0 * bbox_pts[\"h\"] / max(page_h_pts, 1e-6)\n",
        "    return {\"left\": left, \"top\": top, \"width\": width, \"height\": height}\n",
        "\n",
        "def extract_filename_from_url(url: str) -> str:\n",
        "    \"\"\"Extract filename from URL\"\"\"\n",
        "    if url.startswith('s3://'):\n",
        "        parsed = urlparse(url)\n",
        "        filename = os.path.basename(parsed.path)\n",
        "    else:\n",
        "        parsed = urlparse(url)\n",
        "        filename = os.path.basename(parsed.path)\n",
        "\n",
        "    if not filename or filename == '/':\n",
        "        filename = \"unknown.pdf\"\n",
        "\n",
        "    return filename\n",
        "\n",
        "def cardinal_to_weaviate_objects(cardinal_resp: Dict[str, Any],\n",
        "                                 source_url: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"Convert Cardinal response to Weaviate objects.\"\"\"\n",
        "    objs = []\n",
        "    pages = cardinal_resp.get(\"pages\", []) or []\n",
        "\n",
        "    for i in range(len(pages)):\n",
        "        p = pages[i]\n",
        "        page_num = i + 1\n",
        "        page_w_pts = float(p.get(\"width\", 0))\n",
        "        page_h_pts = float(p.get(\"height\", 0))\n",
        "\n",
        "        for bb in p.get(\"bounding_boxes\", []) or []:\n",
        "            bbox_in = bb.get(\"bounding_box\") or {}\n",
        "            content = (bb.get(\"content\") or \"\").strip()\n",
        "            if not content:\n",
        "                continue\n",
        "\n",
        "            bbox_pts = bbox_in_to_pts(bbox_in)\n",
        "            bbox_norm = bbox_pts_to_norm(bbox_pts, page_w_pts, page_h_pts)\n",
        "\n",
        "            # Create properties dictionary - all field names must match the schema exactly\n",
        "            props = {\n",
        "                \"text\": content,\n",
        "                \"type\": \"paragraph\",\n",
        "                \"element_id\": f\"{source_url}#p{page_num}:{hash(content) % (10**9)}\",\n",
        "                \"page_number\": page_num,\n",
        "                \"page_width_pts\": page_w_pts,\n",
        "                \"page_height_pts\": page_h_pts,\n",
        "                \"bbox_in\": {\n",
        "                    \"min_x\": float(bbox_in.get(\"min_x\", 0.0)),\n",
        "                    \"min_y\": float(bbox_in.get(\"min_y\", 0.0)),\n",
        "                    \"max_x\": float(bbox_in.get(\"max_x\", 0.0)),\n",
        "                    \"max_y\": float(bbox_in.get(\"max_y\", 0.0)),\n",
        "                },\n",
        "                \"bbox_pts\": {\n",
        "                    \"x\": float(bbox_pts[\"x\"]),\n",
        "                    \"y\": float(bbox_pts[\"y\"]),\n",
        "                    \"w\": float(bbox_pts[\"w\"]),\n",
        "                    \"h\": float(bbox_pts[\"h\"])\n",
        "                },\n",
        "                \"bbox_norm\": {\n",
        "                    \"left\": float(bbox_norm[\"left\"]),\n",
        "                    \"top\": float(bbox_norm[\"top\"]),\n",
        "                    \"width\": float(bbox_norm[\"width\"]),\n",
        "                    \"height\": float(bbox_norm[\"height\"])\n",
        "                },\n",
        "                \"filename\": extract_filename_from_url(source_url),\n",
        "                \"filetype\": \"pdf\",  # Add filetype field\n",
        "                \"languages\": [\"en\"],  # Add languages field\n",
        "                \"source_url\": source_url,\n",
        "            }\n",
        "\n",
        "            # Create the object with properties key for insert_many\n",
        "            obj = {\"properties\": props}\n",
        "            objs.append(obj)\n",
        "\n",
        "    return objs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7dbqS0BrMjs"
      },
      "source": [
        "## 5) Get S3 URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "p7fYTqHHq413"
      },
      "outputs": [],
      "source": [
        "urls = []\n",
        "\n",
        "if AWS_S3_BUCKET:\n",
        "    s3_path = f\"s3://{AWS_S3_BUCKET}/{S3_PREFIX}\" if not AWS_S3_BUCKET.startswith(\"s3://\") else AWS_S3_BUCKET\n",
        "    urls.extend(list_s3_urls(s3_path, limit=5))  # Start with fewer files for testing\n",
        "\n",
        "print(f\"Found {len(urls)} files to process\")\n",
        "if urls:\n",
        "    print(\"Sample URLs:\", urls[:3])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBns6m4Zreg0"
      },
      "source": [
        "## 6) Process files and collect objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBz3s8g8rfKc",
        "outputId": "f3422037-3533-4c67-e26a-930d6ccbc4d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, ['s3://public-cardinal-bucket/menus/Butterflake Croissant Sandwiches.pdf'])"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Assemble a small list for demo (tweak the limits)\n",
        "urls = []\n",
        "\n",
        "if AWS_S3_BUCKET:\n",
        "    urls.extend(list_s3_urls(AWS_S3_BUCKET if AWS_S3_BUCKET.startswith(\"s3://\") else f\"s3://{AWS_S3_BUCKET}/{S3_PREFIX}\", limit=20))\n",
        "\n",
        "# You can also hardcode URLs (public or presigned)\n",
        "# urls += [\"https://example.com/paper.pdf\"]\n",
        "\n",
        "len(urls), urls[:3]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfTRxbd5t9rr",
        "outputId": "d634b246-fb69-400f-8cab-724e151ccdb0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rProcessing files:   0%|          | 0/1 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing: https://public-cardinal-bucket.s3.us-east-2.amazonaws.com/menus/Butterflake%20Croissant%20Sandwiches.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing files: 100%|██████████| 1/1 [00:20<00:00, 20.89s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted 25 objects from Butterflake Croissant Sandwiches.pdf\n",
            "\n",
            "First object structure:\n",
            "{\n",
            "  \"properties\": {\n",
            "    \"text\": \"BUTTER\\nFLAKE\\nCROISSANT SANDWICHES\",\n",
            "    \"type\": \"paragraph\",\n",
            "    \"element_id\": \"s3://public-cardinal-bucket/menus/Butterflake Croissant Sandwiches.pdf#p1:999507151\",\n",
            "    \"page_number\": 1,\n",
            "    \"page_width_pts\": 612.0,\n",
            "    \"page_height_pts\": 792.0,\n",
            "    \"bbox_in\": {\n",
            "      \"min_x\": 0.6523,\n",
            "      \"min_y\": 0.5002,\n",
            "      \"max_x\": 2.8522,\n",
            "      \"max_y\": 3.1314\n",
            "    },\n",
            "    \"bbox_pts\": {\n",
            "      \"x\": 46.9656,\n",
            "      \"y\": 36.014399999999995,\n",
            "      \"w\": 158.3928,\n",
            "      \"h\": 18...\n",
            "\n",
            "Total objects to insert: 25\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "all_objs = []\n",
        "\n",
        "for raw_url in tqdm(urls, desc=\"Processing files\"):\n",
        "    try:\n",
        "        # Convert S3 URL to HTTPS\n",
        "        https_url = s3_to_https(raw_url) if raw_url.startswith(\"s3://\") else raw_url\n",
        "        if not https_url:\n",
        "            print(f\"Skipping invalid URL: {raw_url}\")\n",
        "            continue\n",
        "\n",
        "        # Process with Cardinal\n",
        "        resp = process_with_cardinal(https_url)\n",
        "\n",
        "        # Debug: Check response structure\n",
        "        if not resp.get(\"pages\"):\n",
        "            print(f\"Warning: No pages found in response for {raw_url}\")\n",
        "            continue\n",
        "\n",
        "        # Convert to Weaviate objects\n",
        "        objects = cardinal_to_weaviate_objects(resp, source_url=raw_url)\n",
        "        print(f\"Extracted {len(objects)} objects from {extract_filename_from_url(raw_url)}\")\n",
        "\n",
        "        # Debug: Print first object structure\n",
        "        if objects and len(all_objs) == 0:  # Only print for first file\n",
        "            print(\"\\nFirst object structure:\")\n",
        "            print(json.dumps(objects[0], indent=2)[:500] + \"...\")\n",
        "\n",
        "        all_objs.extend(objects)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {raw_url}: {e}\")\n",
        "        continue\n",
        "\n",
        "print(f\"\\nTotal objects to insert: {len(all_objs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM6Y4oL28FM2"
      },
      "source": [
        "## 7) Batch insert into Weaviate using the recommended batch method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIK8F-2f5Jyq",
        "outputId": "1b8ad743-92d2-421d-898b-d57f613c719a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Successfully inserted all 25 objects using batch method\n"
          ]
        }
      ],
      "source": [
        "inserted = 0\n",
        "\n",
        "if all_objs:\n",
        "    try:\n",
        "        # Use the batch method which is more reliable\n",
        "        with documents.batch.dynamic() as batch:\n",
        "            for obj in all_objs:\n",
        "                # Extract properties from the object\n",
        "                properties = obj[\"properties\"]\n",
        "                batch.add_object(properties=properties)\n",
        "\n",
        "                # Check for errors periodically\n",
        "                if batch.number_errors > 10:\n",
        "                    print(f\"Stopping due to {batch.number_errors} errors\")\n",
        "                    break\n",
        "\n",
        "        # Check for failed objects\n",
        "        if documents.batch.failed_objects:\n",
        "            print(f\"Failed to insert {len(documents.batch.failed_objects)} objects\")\n",
        "            print(f\"First failed object: {documents.batch.failed_objects[0]}\")\n",
        "        else:\n",
        "            print(f\"Successfully inserted all {len(all_objs)} objects using batch method\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Batch insert error: {e}\")\n",
        "        print(\"Trying insert_many method as fallback...\")\n",
        "\n",
        "        # Fallback to insert_many\n",
        "        BATCH_SIZE = 50  # Smaller batch size\n",
        "        for i in tqdm(range(0, len(all_objs), BATCH_SIZE), desc=\"Inserting to Weaviate\"):\n",
        "            batch = all_objs[i:i+BATCH_SIZE]\n",
        "            try:\n",
        "                response = documents.data.insert_many(batch)\n",
        "                if response.errors:\n",
        "                    print(f\"Errors in batch {i//BATCH_SIZE}: {response.errors}\")\n",
        "                else:\n",
        "                    inserted += len(batch)\n",
        "            except Exception as e:\n",
        "                print(f\"Error inserting batch {i//BATCH_SIZE}: {e}\")\n",
        "\n",
        "        print(f\"Successfully inserted {inserted} objects using insert_many\")\n",
        "else:\n",
        "    print(\"No objects to insert!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVQWQULL8Im-"
      },
      "source": [
        "## 8) Test query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he7iA49VxYyZ",
        "outputId": "665a20a5-1415-4878-c094-753a7acfd052"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Testing Query ===\n",
            "Total documents in collection: 25\n",
            "\n",
            "--- Fetching sample documents to verify structure ---\n",
            "\n",
            "Sample 1:\n",
            "  Text: DRINKS\n",
            "Coffee $3.49\n",
            "Orange Juice $3.49\n",
            "Apple Juice $3.49\n",
            "Milk $3.49...\n",
            "  Filename: Butterflake Croissant Sandwiches.pdf\n",
            "  Page: 1\n",
            "  Type: paragraph\n",
            "  Source URL: s3://public-cardinal-bucket/menus/Butterflake Croissant Sandwiches.pdf\n",
            "  BBox: left=59.9%, top=89.3%, width=10.7%, height=5.7%\n",
            "\n",
            "Sample 2:\n",
            "  Text: # MENU...\n",
            "  Filename: Butterflake Croissant Sandwiches.pdf\n",
            "  Page: 1\n",
            "  Type: paragraph\n",
            "  Source URL: s3://public-cardinal-bucket/menus/Butterflake Croissant Sandwiches.pdf\n",
            "  BBox: left=68.8%, top=42.2%, width=11.2%, height=3.3%\n",
            "\n",
            "--- Hybrid Search Results ---\n",
            "Found 3 results for 'Croissant sandwich':\n",
            "\n",
            "--- Result 1 ---\n",
            "Text: CROISSANT SANDWICHES\n",
            "all served on a toasted Croissant\n",
            "THE STANDARD\n",
            "$9.99\n",
            "Scrambled Egg | American Cheese | Frank's Redhot\n",
            "Add Bacon $\n",
            "Add Sausage $\n",
            "SWEET HAMMY\n",
            "$10.99\n",
            "Scrambled Egg | Smoked Ham | Swi...\n",
            "Filename: Butterflake Croissant Sandwiches.pdf\n",
            "Page: 1\n",
            "Score: 0.8672\n",
            "\n",
            "--- Result 2 ---\n",
            "Text: BUTTER\n",
            "FLAKE\n",
            "CROISSANT SANDWICHES...\n",
            "Filename: Butterflake Croissant Sandwiches.pdf\n",
            "Page: 1\n",
            "Score: 0.7295\n",
            "\n",
            "--- Result 3 ---\n",
            "Text: BREAD: Croissant, 2.5oz DAIRY: American Cheese, Swiss,\n",
            "Feta Crumbles, Milk PROTEIN: Eggs, Sausage Patty, Bacon,\n",
            "Smoked Ham PRODUCE: Iceberg Lettuce, Tomato, Green\n",
            "Pepper, Onion, Spinach CONDIMENTS/SPI...\n",
            "Filename: Butterflake Croissant Sandwiches.pdf\n",
            "Page: 1\n",
            "Score: 0.6735\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== Testing Query ===\")\n",
        "try:\n",
        "    # First, check if there's any data\n",
        "    count_result = documents.aggregate.over_all(total_count=True)\n",
        "    print(f\"Total documents in collection: {count_result.total_count}\")\n",
        "\n",
        "    if count_result.total_count == 0:\n",
        "        print(\"No documents found in collection! Check insertion process.\")\n",
        "    else:\n",
        "        # Method 1: Try a simple fetch first to verify data structure\n",
        "        print(\"\\n--- Fetching sample documents to verify structure ---\")\n",
        "        sample = documents.query.fetch_objects(\n",
        "            limit=2,\n",
        "            include_vector=False\n",
        "        )\n",
        "\n",
        "        if sample.objects:\n",
        "            for i, obj in enumerate(sample.objects, 1):\n",
        "                print(f\"\\nSample {i}:\")\n",
        "                # Access properties directly from the object\n",
        "                if hasattr(obj, 'properties') and obj.properties:\n",
        "                    props = obj.properties\n",
        "                    print(f\"  Text: {props.get('text', 'MISSING')[:100] if props.get('text') else 'MISSING'}...\")\n",
        "                    print(f\"  Filename: {props.get('filename', 'MISSING')}\")\n",
        "                    print(f\"  Page: {props.get('page_number', 'MISSING')}\")\n",
        "                    print(f\"  Type: {props.get('type', 'MISSING')}\")\n",
        "                    print(f\"  Source URL: {props.get('source_url', 'MISSING')}\")\n",
        "\n",
        "                    # Check bbox_norm structure\n",
        "                    bbox_norm = props.get('bbox_norm', {})\n",
        "                    if bbox_norm:\n",
        "                        print(f\"  BBox: left={bbox_norm.get('left', 0):.1f}%, \"\n",
        "                              f\"top={bbox_norm.get('top', 0):.1f}%, \"\n",
        "                              f\"width={bbox_norm.get('width', 0):.1f}%, \"\n",
        "                              f\"height={bbox_norm.get('height', 0):.1f}%\")\n",
        "                else:\n",
        "                    print(f\"  No properties found on object {i}\")\n",
        "                    print(f\"  Object attributes: {dir(obj)}\")\n",
        "\n",
        "        # Method 2: Try hybrid search\n",
        "        print(\"\\n--- Hybrid Search Results ---\")\n",
        "        res = documents.query.hybrid(\n",
        "            query=\"Croissant sandwich\",\n",
        "            alpha=0.5,\n",
        "            limit=3,\n",
        "            include_vector=False,\n",
        "            return_metadata=['score']\n",
        "        )\n",
        "\n",
        "        if res.objects:\n",
        "            print(f\"Found {len(res.objects)} results for 'Croissant sandwich':\")\n",
        "            for i, obj in enumerate(res.objects, 1):\n",
        "                print(f\"\\n--- Result {i} ---\")\n",
        "                if hasattr(obj, 'properties') and obj.properties:\n",
        "                    props = obj.properties\n",
        "                    print(f\"Text: {props.get('text', 'MISSING')[:200] if props.get('text') else 'MISSING'}...\")\n",
        "                    print(f\"Filename: {props.get('filename', 'MISSING')}\")\n",
        "                    print(f\"Page: {props.get('page_number', 'MISSING')}\")\n",
        "\n",
        "                    # Show metadata if available\n",
        "                    if hasattr(obj, 'metadata') and obj.metadata:\n",
        "                        if hasattr(obj.metadata, 'score') and obj.metadata.score is not None:\n",
        "                            print(f\"Score: {obj.metadata.score:.4f}\")\n",
        "                else:\n",
        "                    print(\"No properties returned!\")\n",
        "        else:\n",
        "            print(\"No results found for hybrid search.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Query error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "\n",
        "# Clean up connection\n",
        "client.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
