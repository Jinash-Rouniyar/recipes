{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d85578",
   "metadata": {},
   "source": [
    "# Multimodal Pydantic AI Agent\n",
    "\n",
    "This notebook will show you how to convert base64 images from Weaviate into Pydantic AI's `BinaryContent`.\n",
    "\n",
    "You can then pass these `BinaryContent` inputs to the Pydantic AI `Agent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "001ec952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 17 images to 'pdf_pages'\n"
     ]
    }
   ],
   "source": [
    "import fitz # uv add pymupdf\n",
    "from pathlib import Path\n",
    "\n",
    "PDF = \"Rank1.pdf\" # <-- Replace this with your PDF (If you want to use this PDF, you can get it from https://arxiv.org/abs/2502.18418)\n",
    "OUT_DIR = \"pdf_pages\"\n",
    "DPI = 300 # 300 is \"print-quality\" ...\n",
    "FORMAT = \"png\"\n",
    "\n",
    "doc = fitz.open(PDF)\n",
    "Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "zoom = DPI / 72.0\n",
    "mat = fitz.Matrix(zoom, zoom)\n",
    "\n",
    "for i, page in enumerate(doc, start=1):\n",
    "    pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "    pix.save(Path(OUT_DIR, f\"page_{i:d}.{FORMAT}\"))\n",
    "\n",
    "doc.close()\n",
    "print(f\"Saved {len(list(Path(OUT_DIR).glob(f'*.{FORMAT}')))} images to '{OUT_DIR}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b730b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import base64\n",
    "from pathlib import Path\n",
    "from typing import Tuple, List\n",
    "from pydantic_ai import Agent, BinaryContent\n",
    "\n",
    "def decode_image_from_db(b64_or_data_url: str, fallback_media_type: str = \"image/png\") -> Tuple[bytes, str]:\n",
    "    s = b64_or_data_url.strip()\n",
    "    if s.startswith(\"data:\"):\n",
    "        header, b64 = s.split(\",\", 1)\n",
    "        media_type = header.split(\";\")[0][len(\"data:\"):] or fallback_media_type\n",
    "        return base64.b64decode(b64), media_type\n",
    "    else:\n",
    "        return base64.b64decode(s), fallback_media_type\n",
    "\n",
    "def encode_image(image_path: str) -> str:\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        return base64.b64encode(f.read()).decode(\"utf-8\")\n",
    "\n",
    "def extract_page_num(p: Path) -> int:\n",
    "    m = re.search(r\"page[_-](\\d+)\", p.stem, flags=re.I)\n",
    "    return int(m.group(1)) if m else 10**9\n",
    "\n",
    "def load_binarycontents(paths: List[Path]) -> List[BinaryContent]:\n",
    "    items = []\n",
    "    for p in paths:\n",
    "        b64 = encode_image(str(p)) # simulate Weaviate base64 image storage\n",
    "        img_bytes, media_type = decode_image_from_db(b64, \"image/png\")\n",
    "        items.append(BinaryContent(data=img_bytes, media_type=media_type))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "949d5b63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Sent 2 image(s): pages [1, 2] ===\n",
      "Rank1 was trained on **635,264 examples** of R1 reasoning traces. This number is given towards the end of the \"Data Preparation\" section on page 2:\n",
      "\n",
      "> \"After all generation was done, our dataset has 635,264 examples of R1 generations...\"\n",
      "\n",
      "So, 635,264 reasoning traces from R1 were sourced to train Rank1.\n",
      "This response took 1686 tokens\n",
      "\n",
      "=== Sent 4 image(s): pages [1, 2, 3, 4] ===\n",
      "The number of reasoning traces from R1 that were sourced to train Rank1 is **635,264**.\n",
      "\n",
      "This is found on page 2 in the \"Data Preparation\" section:\n",
      "> \"After all generation was done, our dataset has 635,264 examples of R1 generations...\"\n",
      "This response took 3199 tokens\n",
      "\n",
      "=== Sent 6 image(s): pages [1, 2, 3, 4, 5, 6] ===\n",
      "The number of reasoning traces from R1 that were sourced to train Rank1 is **635,264**.\n",
      "\n",
      "You can find this information on page 2, section \"2.1 Data Preparation\", where it states:\n",
      "> \"After all generation was done, our dataset has 635,264 examples of R1 generations, where R1 labeled 62.9% as relevant and 37.1% as non-relevant.\"\n",
      "This response took 4758 tokens\n",
      "\n",
      "=== Sent 8 image(s): pages [1, 2, 3, 4, 5, 6, 7, 8] ===\n",
      "The number of reasoning traces from R1 that were sourced to train Rank1 is **635,264 examples**. \n",
      "\n",
      "This can be found in Section 2.1 (\"Data Preparation\") on page 2, where it says:\n",
      "\n",
      "> \"After all generation was done, our dataset has 635,264 examples of R1 generations, where R1 labeled 62.9% as relevant and 37.1% as non-relevant.\"\n",
      "\n",
      "So, **635,264 R1 reasoning traces** were sourced for training Rank1.\n",
      "This response took 6309 tokens\n",
      "\n",
      "=== Sent 10 image(s): pages [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] ===\n",
      "The number of reasoning traces from R1 that were sourced to train Rank1 is **635,264**.\n",
      "\n",
      "You can find this on page 2, where it is stated: \n",
      "> \"After all generation was done, our dataset has 635,264 examples of R1 generations...\"\n",
      "\n",
      "This corresponds to the number of R1-generated reasoning chains used for Rank1â€™s training.\n",
      "This response took 7807 tokens\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from pydantic_ai import RunContext\n",
    "\n",
    "class MultimodalDependencies(BaseModel):\n",
    "    prompt: str\n",
    "    all_images: List[Path]\n",
    "\n",
    "agent = Agent(\n",
    "    model=\"openai:gpt-4.1\",\n",
    "    deps_type=MultimodalDependencies\n",
    ")\n",
    "\n",
    "@agent.system_prompt\n",
    "def get_system_prompt(ctx: RunContext[MultimodalDependencies]) -> str:\n",
    "    return f\"You are analyzing document pages. The user will ask questions about content across {len(ctx.deps.all_images)} pages.\"\n",
    "\n",
    "@agent.tool\n",
    "async def get_batch_info(ctx: RunContext[MultimodalDependencies], batch_size: int) -> str:\n",
    "    \"\"\"Get information about the current batch being processed.\"\"\"\n",
    "    return f\"Processing batch of {batch_size} images from directory: {ctx.deps.images_dir}\"\n",
    "\n",
    "async def run_multimodal_qa():\n",
    "    images_dir = Path(\"./pdf_pages\")\n",
    "    batch_sizes = [2, 4, 6, 8, 10]\n",
    "    prompt = \"How many reasoning traces from R1 were sourced to train Rank1?\"\n",
    "    all_pages = sorted(images_dir.glob(\"page_*.png\"), key=extract_page_num)\n",
    "    \n",
    "    deps = MultimodalDependencies(\n",
    "        prompt=prompt,\n",
    "        all_images=all_imgs\n",
    "    )\n",
    "    \n",
    "    for k in batch_sizes:        \n",
    "        batch = all_pages[:k]\n",
    "        contents = load_binarycontents(batch)\n",
    "\n",
    "        result = await agent.run([deps.prompt, *contents], deps=deps)\n",
    "        \n",
    "        page_nums = [extract_page_num(p) for p in batch]\n",
    "        print(f\"\\n=== Sent {k} image(s): pages {page_nums} ===\")\n",
    "        print(result.output)\n",
    "        tokens_used = result.usage().total_tokens\n",
    "        print(f\"This response took {tokens_used} tokens\")\n",
    "\n",
    "await run_multimodal_qa()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
